\documentclass{beamer}
\usetheme{CambridgeUS} 
\usecolortheme{dolphin}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\title[Clustering and Feature Selection]{Model-based Simultaneous Unsupervised Clustering and Feature Selection for Subtype Discovery of Raw RNA-seq Data}
\author{David Lim}
\institute[UNC Chapel Hill]{University of North Carolina, Chapel Hill}
\begin{document}
\SweaveOpts{concordance=TRUE}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Overview}
\tableofcontents
\end{frame}

%%% 1. INTRODUCTION %%%
\section{Introduction}

\begin{frame}{Clustering in Genomics}
\begin{itemize}
%\onslide<+->
\item Grouping of samples (or genes) based on similar expression
%\onslide<+->
\item Similarities and differences between samples can be biologically significant
\item Performed on many different types of data
\end{itemize}
\end{frame}

\begin{frame}{Clustering in Genomics}
\begin{itemize}
%\onslide<+->
\item True number of clusters not known a priori
  \begin{itemize}
  \item Order selection
  \end{itemize}
\item High dimensionality can cause overfitting
  \begin{itemize}
  \item Dimension reduction/variable selection
  \end{itemize}
\item Noise can be mixed in with biological variation
\item Confounding factors to differential expression
\end{itemize}
\end{frame}

\begin{frame}{Implications in Cancer Genomics}
\begin{itemize}
\item Samples grouped together can imply shared subtype of cancer
  \begin{itemize}
  \item Examine different prognoses of subtypes
  \item Targeted therapy for more effective treatment
  \end{itemize}
\item Informative genes in clustering
  \begin{itemize}
  \item Isolate potential driving genes
  \item Low false positive rate: decrease need for costly investigation
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{RNA-seq Data}
\begin{itemize}
\item We propose using raw RNA-seq count data to cluster. Why?
  \begin{itemize}
  \item Less noise and greater dynamic range of detection than microarray (Zhao 2014, Hrdlickova 2016)
  \item Independent of any normal-approximating transformation: not one uniformly superior (Noel-MacDonnell 2018)
  \end{itemize}
\item No true method designed specifically for this data
\item Normalization step to adjust for sequencing depth
  \begin{itemize}
  \item Most normalization methods (edgeR, DESeq2) are model-based
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Some Existing Methods}
\begin{itemize}
\item Not many methods for clustering samples with count data
\item iCluster+ (Mo 2013)
  \begin{itemize}
  \item Integrative method
  \item Assumes Poisson distribution of count data
  \end{itemize}
\item Hierarchical clustering and K-medoids (Jaskowiak 2018)
\end{itemize}
\end{frame}

\begin{frame}{Proposed Method}
\begin{itemize}
%\onslide<+->
\item Finite Mixture Negative Binomial Model
  \begin{itemize}
  %\onslide<+->
  \item Better handling of overdispersed count data
  \item Model-based clustering seamless integration with most normalization methods (edgeR, DESeq2)
  \end{itemize}
%\onslide<+->
\item Expectation-Maximization Algorithm
  \begin{itemize}
  %\onslide<+->
  \item Maximization of likelihood-based objective function
  %\onslide<+->
  \item Penalty to prevent overfitting and for selection
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Key Features of Our Method}
\begin{itemize}
\item Simultaneous sample clustering and feature selection
\item Model-based framework consistent with widely-used normalization methods
\item Cluster prediction of new samples based on fitted model
\item Can easily adjust for effects of known biases into existing algorithm
  \begin{itemize}
  \item Batch effects, gender, age, etc.
  \end{itemize}
\end{itemize}
\end{frame}

%%% 2. METHODS %%%
\section{Methods}
\subsection{Likelihood Model}

\begin{frame}{Finite Mixture Model}
\begin{equation}
f(\mathbf{y};\boldsymbol{\mu},\boldsymbol{\phi}) = \prod_{i=1}^n \prod_{j=1}^g \sum_{k=1}^K \pi_k f_{jk}(y_{ij};\mu_{jk},\phi_j)
\end{equation}
\begin{itemize}
\item Negative Binomial distribution
\item n samples
\item g genes
\item K clusters
\end{itemize}
\end{frame}

\begin{frame}{Penalty}
\begin{itemize}
%\onslide<+->
\item Imposed on difference in cluster log means $\abs{\theta_{kl}}$
%\onslide<+->
\item SCAD elastic net penalty:
%\onslide<+->
\end{itemize}
$$
p_{\alpha,\lambda}(\abs{\theta_{kl}})=(1-\alpha)p_1(\abs{\theta_{kl}}) + \alpha p_2(\abs{\theta_{kl}})
$$
\begin{itemize}
%\onslide<+->
\item $p_1$: classic L2 (ridge) penalty
  \begin{itemize}
  \item Efficiently shrinks cluster log means towards each other
  \item Can't set log means equal ($\theta_{kl}=0$)
  \end{itemize}
\onslide<+->
\item $p_2$: SCAD penalty
  \begin{itemize}
  \item Sets cluster log means equal when close enough
  \item More adaptive: introduces less bias than L1 (lasso) penalty
  \end{itemize}
\end{itemize}

\end{frame}

\subsection{Computation}

\begin{frame}{Expectation-Maximization Algorithm}
%\onslide<+->
E step: Uses parameter estimates and output posterior probabilities \\~\\
\begin{equation}
w_{ik}^{(m)}=\dfrac{\hat{\pi}_k^{(m)}f_k(\boldsymbol{y_i};\boldsymbol{\hat{\beta}_k^{(m)},\hat{\phi}_k^{(m)}})}{\sum_{l=1}^{K}\hat{\pi}_l^{(m)}f_l(\boldsymbol{y_i};\boldsymbol{\hat{\beta}_l^{(m)},\hat{\phi}_l^{(m)}})}
\end{equation} \\~\\
%\onslide<+->
M step: Maximizes parameters using E step weights
\end{frame}

\begin{frame}{Tuning Parameters}
\begin{itemize}
\item Optimal parameters found by minimizing BIC
%\onslide<+->
\item Order Selection (OS)
  \begin{itemize}
  \item Unpenalized runs of EM
  \item Search over range of possible numbers of clusters
  \end{itemize}
%\onslide<+->
\item Penalty parameters
  \begin{itemize}
  \item Input optimal order from OS step
  \item Search over grid of combinations of values
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Order Selection}
\begin{figure}
\begin{center}
%\includegraphics{C:/Users/limdd/Documents/Research/Sweave/Project1/BICvOrder.png}
<<echo=FALSE,fig=TRUE,height=4,width=5>>=
load("C:/Users/limdd/Documents/Research/Sweave/Project1/list_BIC_n100_g865_K3.Rout")
plot(list_BIC,ylab="BIC",xlab="K",main="Plot of BIC vs. order",type='l')
@
\end{center}
\caption{Order selection is done by choosing the order (K) that minimizes the BIC. Here, the true order is K = 3 in a simulated dataset of n = 100 and g = 835 (after pre-filtering low count genes), and 20 \% discriminatory genes with log fold change of 2.}
\label{fig:1}
\end{figure}
\end{frame}

\begin{frame}{Prediction}
\begin{itemize}
\item Use coefficient and dispersion estimates to predict new sample's subtype
\end{itemize}
\end{frame}

%%% 3. Simulations %%%
\section{Simulations}
\begin{frame}{Set-up}
Fixed: $g=2000, p_{disc}=0.05$ \\~\\
Variable:\\
$\beta=(3.75, 6.5)$ \\
$\phi=(0.15, 0.35)$  \\
$K=(2,4)$ \\
$n_{per}=(25,50)$ \\
$LFC=(1,2)$ \\~\\
\end{frame}

<<echo=FALSE,print=FALSE>>=
library(xtable)
load("C:/Users/limdd/Documents/Research/Simulations/elasticSCAD/24sims/sim_res.RData")

#head(table1)
colnames(table1) = c("$\\beta$","$\\phi$","LFC","n","K","$K_{FSC}$","$OA_{FSC}$","$p_{disc}$","$\\hat{p}_{disc}$","$ARI_{FSC}$","sens","FPR","$sens_{pre}$","$FPR_{pre}$","PA","$K_{iCl}$","$OA_{iCl}$","$ARI_{iCl}$","$K_{HC}$","$OA_{HC}$","$ARI_{HC}$","$K_{KM}$","$OA_{KM}$","$ARI_{KM}$","$K_{NBMB}$","$OA_{NBMB}$","$ARI_{NBMB}$","$K_{lMC}$","$OA_{lMC}$","$ARI_{lMC}$","$K_{vMC}$","$OA_{vMC}$","$ARI_{vMC}$","$K_{rMC}$","$OA_{rMC}$","$ARI_{rMC}$")

table1_EM_15=table1[c(1:24,97:120),c(1:13,15)][,-2]
table1_EM_35=table1[c(25:48,121:144),c(1:13,15)][,-2]
table1_EM_50=table1[c(49:72,145:168),c(1:13,15)][,-2]
table1_EM_100=table1[c(73:96,169:192),c(1:13,15)][,-2]

table2 = table1[table1[,"$p_{disc}$"]==0.05,]

table2_other_order_15=table2[c(1:12,49:60),c(1:6,16,19,22,25,28,31,34)][,-2]
table2_other_order_35=table2[c(13:24,61:72),c(1:6,16,19,22,25,28,31,34)][,-2]
table2_other_order_50=table2[c(25:36,73:84),c(1:6,16,19,22,25,28,31,34)][,-2]
table2_other_order_100=table2[c(37:48,85:96),c(1:6,16,19,22,25,28,31,34)][,-2]

table2_other_OA_15=table2[c(1:12,49:60),c(1:5,7,17,20,23,26,29,32,35)][,-2]
table2_other_OA_35=table2[c(13:24,61:72),c(1:5,7,17,20,23,26,29,32,35)][,-2]
table2_other_OA_50=table2[c(25:36,73:84),c(1:5,7,17,20,23,26,29,32,35)][,-2]
table2_other_OA_100=table2[c(37:48,85:96),c(1:5,7,17,20,23,26,29,32,35)][,-2]

table2_other_ARI_15=table2[c(1:12,49:60),c(1:5,10,18,21,24,27,30,33,36)][,-2]
table2_other_ARI_35=table2[c(13:24,61:72),c(1:5,10,18,21,24,27,30,33,36)][,-2]
table2_other_ARI_50=table2[c(25:36,73:84),c(1:5,10,18,21,24,27,30,33,36)][,-2]
table2_other_ARI_100=table2[c(37:48,85:96),c(1:5,10,18,21,24,27,30,33,36)][,-2]

table2=table1[table1$K==2,]
table4=table1[table1$K==4,]
table_high_phi = table1[table1[,2]>=0.5,]
table2_high_phi = table_high_phi[table_high_phi$K==2,]
table4_high_phi = table_high_phi[table_high_phi$K==4,]
@

\begin{frame}{MAD Pre-filtering}
\begin{center}
<<echo=FALSE,fig=TRUE,height=4,width=8>>=
x1=table1$'$sens_{pre}$'[table1[,2]==0.15]
x2=table1$'$sens_{pre}$'[table1[,2]==0.35]
x3=table1$'$sens_{pre}$'[table1[,2]==0.50]
x4=table1$'$sens_{pre}$'[table1[,2]==1.00]
colors = table1$K[table1[,2]==0.15]

plot(x=c(0.15,0.35,0.5,1),y=c(x1[1],x2[1],x3[1],x4[1]),type='b',col=colors[1],xlab=expression(phi),ylab="Sensitivity",
     ylim=c(.2,1),main=expression(paste("Plot of Sensitivity vs. ", phi," of Top 20% MAD Pre-filtering of Genes")))
for(i in 2:length(x1)){
  points(x=c(0.15,0.35,0.5,1),y=c(x1[i],x2[i],x3[i],x4[i]),type='b',col=colors[i])
}
legend("bottomleft",col=c("red","blue"),legend=c("K=2","K=4"),lty=1)

# boxplot(table1$Filt_sens[table1[,2]==0.15],
#         table1$Filt_sens[table1[,2]==0.35],
#         table1$Filt_sens[table1[,2]==0.50],
#         table1$Filt_sens[table1[,2]==1.00],
#         names=c("0.15","0.35","0.50","1.00"),xlab="Phi",ylab="Sensitivity",main="Boxplot of Sensitivity of Top 20\\% MAD Pre-filtering of Genes")
@
\end{center}
\end{frame}

\begin{frame}{Low noise = great clustering performance}
<<xtable1, results=tex, echo=FALSE>>=
tab1=xtable(rbind(table2[,c("$\\beta$","$\\phi$","K","OA","ARI")][c(1,5,17,21),],
                  table4[,c("$\\beta$","$\\phi$","K","OA","ARI")][c(1,5,17,21),]),
            digits=3,
            table.placement="!h",
            caption="Performance metrics at low to moderate noise. Performance is perfect through all simulated datasets",
            label = "tab:1",na.print="")
print(tab1,include.rownames=F, sanitize.text.function=identity)
@
Perfect order selection and ARI. $n_{K}=25$, $LFC=1$
\end{frame}

\begin{frame}{Increasing n improves Order Selection}
<<xtable2, results=tex, echo=FALSE>>=
tab2=xtable(table4_high_phi[5:8,c("LFC","$n_{K}$","K","OA","ARI")],
            digits=3,
            table.placement="!h",
            caption="Effect of increasing n: better order accuracy and ARI",
            label = "tab:2",na.print="")
print(tab2,include.rownames=F, sanitize.text.function=identity)
@
$true.K=4, \beta=3.75, \phi=1$ (high noise)
\end{frame}

\begin{frame}{Gene discovery with high noise = better with higher $\beta$}
<<xtable3, results=tex, echo=FALSE>>=
tab3=xtable(table1[c(9,41,10,42,17,49,18,50),c("$\\beta$","$\\phi$","$n_{K}$","$p_{disc}$","sens","false.pos")],
            digits=3,
            table.placement="!h",
            caption="Sensitivity and FPR both generally improve as LFC increases, especially in noisier datasets. Higher K => more samples => better gene discovery performance",
            label = "tab:3",na.print="")
print(tab3,include.rownames=F, sanitize.text.function=identity)
@
$true.K=2, LFC=1$
\end{frame}

\begin{frame}{Increasing K or LFC = better gene discovery}
<<xtable4, results=tex, echo=FALSE>>=
tab4=xtable(table1[c(17,19,18,20,21,23,22,24),c("true.K","$n_{K}$","$p_{disc}$","sens","false.pos")],
            digits=3,
            table.placement="!h",
            caption="Sensitivity and FPR both generally improve as LFC increases, especially in the higher noise setting ($\\phi=0.5$) ",
            label = "tab:4",na.print="")
print(tab4,include.rownames=F, sanitize.text.function=identity)
@
$\beta=3.75, \phi=0.5$
\end{frame}

\begin{frame}{Increasing $\phi$ decreases sensitivity of gene discovery}
<<xtable5, results=tex, echo=FALSE>>=
tablephi1=table1[rep(1:8,each=4)+rep(c(0,8,16,24),times=8),c("$\\beta$","$\\phi$","LFC","$n_{K}$","true.K","K","OA","ARI","$p_{disc}$","sens","false.pos")]
tablephi2=table1[rep(1:8,each=4)+rep(c(0,8,16,24)+32,times=8),c("$\\beta$","$\\phi$","LFC","$n_{K}$","true.K","K","OA","ARI","$p_{disc}$","sens","false.pos")]
tab5=xtable(tablephi1[5:8,c("$\\phi$","$p_{disc}$","sens","false.pos")],
            digits=3,
            table.placement="!h",
            caption="Effect of increased simulated noise on gene discovery. Performance decreases, as it is harder to distinguish differences between clusters in noisier datasets",
            label = "tab:5",na.print="")
print(tab5,include.rownames=F, sanitize.text.function=identity)
@
$n_K=50, LFC=1, true.K=2, \beta=3.75$
\end{frame}

\begin{frame}{Increasing $\phi$ decreases clustering performance}
<<xtable6, results=tex, echo=FALSE>>=
tab6=xtable(tablephi1[c(21:24),c("$\\phi$","K","OA","ARI","sens","false.pos")],
            digits=3,
            table.placement="!h",
            caption="Effect of increased simulated noise on clustering performance. Clustering performance was generally very good, and began to show inaccuracies only at very high simulated noise ($\phi=1.00$)",
            label = "tab:6",na.print="")
print(tab6,include.rownames=F, sanitize.text.function=identity)
@
$n_K=50, LFC=1, true.K=4, \beta=3.75$
\end{frame}

\begin{frame}{Order Selection = very important!}
<<xtable7, results=tex, echo=FALSE>>=
tab7=xtable(table1[table1[,"ARI"]==table1[,"OA"] & table1[,"OA"]!=1,c("$\\beta$","$\\phi$","LFC","$n_{K}$","K","OA","ARI","sens")],
            digits=3,
            table.placement="!h",
            caption="Performance of clustering and gene discovery are highly dependent on the order being selected correctly. Many cases where performance is perfect when order is correctly selected, while very bad performance if order is incorrectly selected.",
            label = "tab:7",na.print="")
print(tab7,include.rownames=F, sanitize.text.function=identity)
@
$true.K=2$
\end{frame}

\begin{frame}{Cluster Prediction Performance}     % Prediction accuracy was 100% for correctly selected orders, no matter what other params?!
\begin{itemize}
\item Test set: $0.1n$ simulated, based on same parameters as training set
\item Predictions performed just on simulation runs that correctly selected the order
\item Clustering on newly simulated samples, based on fitting parameter estimates
  \begin{itemize}
  \item $100\%$ accuracy of subtype prediction when order was correctly selected
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Order Selection of Other Methods}
<<xtable8, results=tex, echo=FALSE>>=
tab8 = xtable(table1[c(61:64),c("LFC","$n_{K}$","K","iK","$K_{hc}$","$K_{med}$")],
            digits=3,
            table.placement="!h",
            caption="Select examples of order selected by iCluster+, HC, and K-medoids. In general, order selection by BIC with EM seems to perform much better than other methods. Note: K-medoids could not search K=1.",
            label = "tab:8",na.print="")
print(tab8,include.rownames=F, sanitize.text.function=identity)
@
$\beta=6.50, \phi=1.00, true.K=4$
\end{frame}

\begin{frame}{Clustering Performance of Other Methods}
<<xtable9, results=tex, echo=FALSE>>=
tab9 = xtable(table1[c(61:64),c("LFC","$n_{K}$","ARI","iARI","$ARI_{hc}$","$ARI_{med}$")],
            digits=3,
            table.placement="!h",
            caption="Select examples of clustering performance by iCluster+, HC and K-medoids. iCluster+ acts sporadically because of a lack of a systemized way of selecting the order. EM generally seems to outperform the other methods in clustering performance.",
            label = "tab:9",na.print="")
print(tab9,include.rownames=F, sanitize.text.function=identity)
@
$\beta=6.50, \phi=1.00, true.K=4$
\end{frame}

%%% 4. Real Data %%%
\section{Real Data}

\begin{frame}{TCGA BRCA}
\begin{itemize}
\item Excluded Normal-like subtype ($n=8$)
\item Pre-filtered low count genes, and selected top 10\%, top 1000 median absolute deviation (MAD) genes
  \begin{itemize}
  \item Only 30 of 50 PAM50 genes were in the top 25\% MAD
  \end{itemize}
\item 4 subtypes, $n=513$, $g=1969$
\item Clinical survival: 62 OS (overall survival) events used as survival event. Nonmissing survival times from 79 subjects. NOTE: Using vital status (79) gave very different pvalues (all non-significant)
  \begin{itemize}
  \item Not many survival endpoints to analyze.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Permutation-based Survival}
\begin{itemize}
\item Log-rank test based on Chi-square approximation widely used
\item Approximation may not be appropriate (Rappoport 2018)
  \begin{itemize}
  \item Log-rank p-values based on empirical and Chi-square tests differed greatly
  \item Empirical p-value can overcome bias when small #samples per cluster
  \item Approximate p-values were always more significant
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{TCGA BRCA Results}
<<xtable10,results=tex,echo=F>>=
load("C:/Users/limdd/Documents/Research/Sweave/Presentation102618/Results/BRCA_table.RData")
BRCA_tab = summary_table[,c(9,1:5)]
BRCA_tab[1,2]=NA
tab10 = xtable(BRCA_tab,digits=3,caption="Cluster analyses and clinical analyses in clustering derived by CSEM, iCluster, HC, Kmed using top 10\\% and top 1000 MAD genes. Empirical pvalue unavailable for clustering with more than 95\\% of samples in the same cluster due to sampling issues.",
              label="tab:10",table.placement="H",na.print="",
              floating.environment = "table")
print(tab10,sanitize.text.function=identity,table.placement="H")
@
\end{frame}

\begin{frame}{TCGA BLCA}
\begin{itemize}
\item Included less frequent subtypes Neuronal ($n=20$) and Luminal ($n=26$)
\item Pre-filtered low count genes, and selected top 10\% median absolute deviation (MAD) genes
\item 5 subtypes, $n=408$, $g=1967$
\item Clinical survival: 180 survival events of 408 samples, nonmissing survival times from 178 of these events.
\end{itemize}
\end{frame}

\begin{frame}{TCGA BLCA Results}
<<xtable11,results=tex,echo=F>>=
load("C:/Users/limdd/Documents/Research/Sweave/Presentation102618/Results/BLCA_table.RData")
BLCA_tab = summary_table[,c(10,1:5)]
BLCA_tab[1,2]=NA
tab11 = xtable(BLCA_tab,digits=3,caption="Cluster analyses and clinical analyses in clustering derived by CSEM, iCluster, HC, Kmed using top 10\\% and top 1000 MAD genes. Empirical pvalue unavailable for clustering with more than 95\\% of samples in the same cluster due to sampling issues.",
              label="tab:11",table.placement="H",na.print="",
              floating.environment = "table")
print(tab11,sanitize.text.function=identity,table.placement="H")
@
\end{frame}

%%% 5. Discussion %%%
\section{Discussion}
\begin{frame}{Conclusions}
\begin{itemize}
\item CSEM outperforms competitors in clustering RNA-seq in simulations, and challenges methods already used in real data
\item Gene discovery performance dependent on cluster separation (LFC)
\item Order selection problem difficult but very significant
\item It may be better to simulate cluster separation (LFC) adaptive to the cluster log mean
\item As amount of samples ($n$) increases, clustering performance will increase
\item Chi-square approximation Log-rank test used to measure efficacy of clustering needs to be questioned
\end{itemize}
\end{frame}

\begin{frame}{Future Directions}
\begin{itemize}
\item Adjust for purity and other known biases
\item Incorporate gene-to-gene correlation
\item Extend to integrative analysis of multiple omics
\end{itemize}
\end{frame}

\section*{Acknowledgments}
\begin{frame}{Special Thanks}
Dr. Naim Rashid, Dr. Joseph Ibrahim
\end{frame}

\end{document}