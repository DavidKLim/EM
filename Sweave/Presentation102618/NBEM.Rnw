\documentclass{beamer}
\usetheme{CambridgeUS} 
\usecolortheme{dolphin}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{caption}
\captionsetup{font=footnotesize}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\title[Clustering and Feature Selection]{Model-based Simultaneous Unsupervised Clustering and Feature Selection for Subtype Discovery of Raw RNA-seq Data}
\author{David Lim}
\institute[UNC Chapel Hill]{University of North Carolina, Chapel Hill}
\begin{document}
\SweaveOpts{concordance=TRUE}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Overview}
\tableofcontents
\end{frame}

%%% 1. INTRODUCTION %%%
\section{Introduction}

\begin{frame}{Clustering in Genomics}
\begin{itemize}
%\onslide<+->
\item Grouping of samples (or genes) based on similar expression
%\onslide<+->
\item Similarities and differences between samples can be biologically significant
\item Performed on many different types of data
\end{itemize}
\end{frame}

\begin{frame}{Clustering in Genomics}
\begin{itemize}
%\onslide<+->
\item True number of clusters not known a priori
  \begin{itemize}
  \item Order selection
  \end{itemize}
\item High dimensionality can cause overfitting
  \begin{itemize}
  \item Dimension reduction/variable selection
  \end{itemize}
\item Noise can be mixed in with biological variation
\item Confounding factors to differential expression
\end{itemize}
\end{frame}

\begin{frame}{Implications in Cancer Genomics}
\begin{itemize}
\item Samples grouped together can imply shared subtype of cancer
  \begin{itemize}
  \item Examine different prognoses of subtypes
  \item Targeted therapy for more effective treatment
  \end{itemize}
\item Informative genes in clustering
  \begin{itemize}
  \item Isolate potential driving genes
  \item Low false positive rate: decrease need for costly investigation
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{RNA-seq Data}
\begin{itemize}
\item We propose using raw RNA-seq count data to cluster. Why?
  \begin{itemize}
  \item Less noise and greater dynamic range of detection than microarray (Zhao 2014, Hrdlickova 2016)
  \item Independent of any normal-approximating transformation: not one uniformly superior (Noel-MacDonnell 2018)
  \end{itemize}
\item Normalization step to adjust for sequencing depth
  \begin{itemize}
  \item Most normalization methods (edgeR, DESeq2) are model-based
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Some Existing Methods}
\begin{itemize}
\item Not many methods for clustering samples with count data
\item iCluster+ (Mo 2013)
  \begin{itemize}
  \item Integrative method
  \item Assumes Poisson distribution of count data
  \end{itemize}
\item Hierarchical clustering and K-medoids (Jaskowiak 2018)
\item NB.MClust (Li 2018)
\item mclust (Scrucca 2016) on transformed data
\end{itemize}
\end{frame}

\begin{frame}{Proposed Method}
\begin{itemize}
%\onslide<+->
\item Finite Mixture Negative Binomial Model
  \begin{itemize}
  %\onslide<+->
  \item Better handling of overdispersed count data
  \item Model-based clustering seamless integration with most normalization methods (edgeR, DESeq2)
  \end{itemize}
%\onslide<+->
\item Expectation-Maximization Algorithm
  \begin{itemize}
  %\onslide<+->
  \item Maximization of likelihood-based objective function
  %\onslide<+->
  \item Penalty to prevent overfitting and for selection
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Key Features of Our Method}
\begin{itemize}
\item Simultaneous sample clustering and feature selection
\item Model-based framework consistent with widely-used normalization methods
\item Cluster prediction of new samples based on fitted model
\item Can potentially adjust for effects of known biases into existing algorithm
  \begin{itemize}
  \item Batch effects, gender, age, etc.
  \end{itemize}
\end{itemize}
\end{frame}

%%% 2. METHODS %%%
\section{Methods}
\subsection{Likelihood Model}

\begin{frame}{Finite Mixture Model}
\begin{equation}
f(\mathbf{y};\boldsymbol{\mu},\boldsymbol{\phi}) = \prod_{i=1}^n \prod_{j=1}^g \sum_{k=1}^K \pi_k f_{jk}(y_{ij};\mu_{jk},\phi_j)
\end{equation}
\begin{itemize}
\item Negative Binomial distribution
\item n samples
\item g genes
\item K clusters
\end{itemize}
\end{frame}

\begin{frame}{Penalty}
\begin{itemize}
%\onslide<+->
\item Imposed on difference in cluster log means $\abs{\theta_{kl}}$
%\onslide<+->
\item SCAD elastic net penalty:
%\onslide<+->
\end{itemize}
$$
p_{\alpha,\lambda}(\abs{\theta_{kl}})=(1-\alpha)p_1(\abs{\theta_{kl}}) + \alpha p_2(\abs{\theta_{kl}})
$$
\begin{itemize}
%\onslide<+->
\item $p_1$: classic L2 (ridge) penalty
  \begin{itemize}
  \item Efficiently shrinks cluster log means towards each other
  \item Can't set log means equal ($\theta_{kl}=0$)
  \end{itemize}
\onslide<+->
\item $p_2$: SCAD penalty
  \begin{itemize}
  \item Sets cluster log means equal when close enough
  \item More adaptive: introduces less bias than L1 (lasso) penalty
  \end{itemize}
\end{itemize}

\end{frame}

\subsection{Computation}

\begin{frame}{Expectation-Maximization Algorithm}
%\onslide<+->
E step: Uses parameter estimates and output posterior probabilities \\~\\
\begin{equation}
w_{ik}^{(m)}=\dfrac{\hat{\pi}_k^{(m)}f_k(\boldsymbol{y_i};\boldsymbol{\hat{\beta}_k^{(m)},\hat{\phi}_k^{(m)}})}{\sum_{l=1}^{K}\hat{\pi}_l^{(m)}f_l(\boldsymbol{y_i};\boldsymbol{\hat{\beta}_l^{(m)},\hat{\phi}_l^{(m)}})}
\end{equation} \\~\\
%\onslide<+->
M step: Maximizes parameters using E step weights
\end{frame}

\begin{frame}{Tuning Parameters}
\begin{itemize}
\item Optimal parameters found by minimizing BIC
%\onslide<+->
\item Order Selection (OS)
  \begin{itemize}
  \item Unpenalized runs of EM
  \item Search over range of possible numbers of clusters
  \end{itemize}
%\onslide<+->
\item Penalty parameters
  \begin{itemize}
  \item Input optimal order from OS step
  \item Search over grid of combinations of values
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Order Selection}
\begin{figure}
\begin{center}
%\includegraphics{C:/Users/limdd/Documents/Research/Sweave/Project1/BICvOrder.png}
<<echo=FALSE,fig=TRUE,height=4,width=5>>=
load("C:/Users/limdd/Documents/Research/Sweave/Project1/list_BIC_n100_g865_K3.Rout")
plot(list_BIC,ylab="BIC",xlab="K",main="Plot of BIC vs. order",type='l')
@
\end{center}
\caption{Order selection is done by choosing the order (K) that minimizes the BIC. Here, the true order is K = 3 in a simulated dataset of n = 100 and g = 835 (after pre-filtering low count genes), and 20 \% discriminatory genes with log fold change of 2.}
\label{fig:1}
\end{figure}
\end{frame}

\begin{frame}{Prediction}
\begin{itemize}
\item Use coefficient and dispersion estimates to predict new sample's subtype
\end{itemize}
\end{frame}

%%% 3. Simulations %%%
\section{Simulations}
\begin{frame}{Set-up}
Fixed: $g=2000$ \\~\\
Variable:\\
$\beta=(3.75, 6.5)$ \\
$\phi=(0.15, 0.35, 0.50, 1.00)$  \\
$K=(2, 4, 6)$ \\
$n=(100, 200)$ \\
$p_{disc}=0.05$ \\
$LFC=(1, 2)$ \\~\\
\end{frame}

<<echo=FALSE,print=FALSE>>=
library(xtable)
  load("C:/Users/limdd/Documents/Research/Simulations/elasticSCAD/24sims/sim_res.RData")

table1[,9] = table1[,9]*2000       # number of disc genes g_{disc}
#head(table1)
colnames(table1) = c("$\\beta$","$\\phi$","LFC","n","K","$K_{FSC}$","$OA_{FSC}$","$p_{disc}$","$g_{disc}$","$ARI_{FSC}$","sens","FPR","$sens_{pre}$","$FPR_{pre}$","PA","$K_{iCl}$","$OA_{iCl}$","$ARI_{iCl}$","$K_{HC}$","$OA_{HC}$","$ARI_{HC}$","$K_{KM}$","$OA_{KM}$","$ARI_{KM}$","$K_{NBMB}$","$OA_{NBMB}$","$ARI_{NBMB}$","$K_{lMC}$","$OA_{lMC}$","$ARI_{lMC}$","$K_{vMC}$","$OA_{vMC}$","$ARI_{vMC}$","$K_{rMC}$","$OA_{rMC}$","$ARI_{rMC}$")

table1_EM_15=table1[c(1:24,97:120),c(1:13,15)][,-2]
table1_EM_35=table1[c(25:48,121:144),c(1:13,15)][,-2]
table1_EM_50=table1[c(49:72,145:168),c(1:13,15)][,-2]
table1_EM_100=table1[c(73:96,169:192),c(1:13,15)][,-2]

table2 = table1[table1[,"$p_{disc}$"]==0.05,]

table2_other_order_15=table2[c(1:12,49:60),c(1:6,16,19,22,25,28,31,34)][,-2]
table2_other_order_35=table2[c(13:24,61:72),c(1:6,16,19,22,25,28,31,34)][,-2]
table2_other_order_50=table2[c(25:36,73:84),c(1:6,16,19,22,25,28,31,34)][,-2]
table2_other_order_100=table2[c(37:48,85:96),c(1:6,16,19,22,25,28,31,34)][,-2]

table2_other_OA_15=table2[c(1:12,49:60),c(1:5,7,17,20,23,26,29,32,35)][,-2]
table2_other_OA_35=table2[c(13:24,61:72),c(1:5,7,17,20,23,26,29,32,35)][,-2]
table2_other_OA_50=table2[c(25:36,73:84),c(1:5,7,17,20,23,26,29,32,35)][,-2]
table2_other_OA_100=table2[c(37:48,85:96),c(1:5,7,17,20,23,26,29,32,35)][,-2]

table2_other_ARI_15=table2[c(1:12,49:60),c(1:5,10,18,21,24,27,30,33,36)][,-2]
table2_other_ARI_35=table2[c(13:24,61:72),c(1:5,10,18,21,24,27,30,33,36)][,-2]
table2_other_ARI_50=table2[c(25:36,73:84),c(1:5,10,18,21,24,27,30,33,36)][,-2]
table2_other_ARI_100=table2[c(37:48,85:96),c(1:5,10,18,21,24,27,30,33,36)][,-2]

table2=table1[table1$K==2,]
table4=table1[table1$K==4,]
table_high_phi = table1[table1[,2]>=0.5,]
table2_high_phi = table_high_phi[table_high_phi$K==2,]
table4_high_phi = table_high_phi[table_high_phi$K==4,]
@

\begin{frame}{MAD Pre-filtering}
\begin{fig}
\begin{center}
<<echo=FALSE,fig=TRUE,height=4.8,width=8>>=
x1=table1$'$sens_{pre}$'[table1[,2]==0.15 & table1$K==2]
x2=table1$'$sens_{pre}$'[table1[,2]==0.35 & table1$K==2]
x3=table1$'$sens_{pre}$'[table1[,2]==0.50 & table1$K==2]
x4=table1$'$sens_{pre}$'[table1[,2]==1.00 & table1$K==2]

colors = 1:5

select = c(2,3,5,9)

plot(x=c(0.15,0.35,0.5,1),y=c(x1[1],x2[1],x3[1],x4[1]),type='b',col=colors[1],xlab=expression(phi),ylab="Sensitivity",
     ylim=c(.25,1),main=expression(paste("Plot of Sensitivity vs. ", phi," of Top 20% MAD Pre-filtering of Genes")))
for(i in 1:length(select)){
  points(x=c(0.15,0.35,0.5,1),y=c(x1[select[i]],x2[select[i]],x3[select[i]],x4[select[i]]),type='b',col=colors[i+1])
}
legend("bottomleft",col=1:5,legend=c("base","n=200",expression(paste(p[disc],"=0.1")),"LFC=2",expression(paste(beta,"=6.5"))),lty=1)

# boxplot(table1$Filt_sens[table1[,2]==0.15],
#         table1$Filt_sens[table1[,2]==0.35],
#         table1$Filt_sens[table1[,2]==0.50],
#         table1$Filt_sens[table1[,2]==1.00],
#         names=c("0.15","0.35","0.50","1.00"),xlab="Phi",ylab="Sensitivity",main="Boxplot of Sensitivity of Top 20\\% MAD Pre-filtering of Genes")
@
\end{center}
\end{figure}
Plot of MAD pre-filtering sensitivity of discovering discriminatory genes vs noise at $K=2$. Base case (base) is $\beta=3.75$, $n=100$, $p_{disc}=0.05$, $LFC=1$.
\end{frame}

\begin{frame}{Increasing n improves Clustering Performance}
<<xtable2, results=tex, echo=FALSE>>=
tab1=xtable(table4_high_phi[1:4,c("n","$p_{disc}$","$K_{FSC}$","$OA_{FSC}$","$ARI_{FSC}$")],
            digits=3,
            table.placement="!h",
            caption="Effect of increasing n: better order accuracy and ARI",
            label = "tab:1",na.print="")
print(tab1,include.rownames=F, sanitize.text.function=identity)
@
$\beta=3.75, \phi=1, K=4, LFC=1$
\end{frame}

\begin{frame}{Gene discovery = better with higher $\beta$}
<<xtable3, results=tex, echo=FALSE>>=
# table1[rep(1:96,each=2)+rep(c(0,96),times=96),c("$\\beta$","$\\phi$","n","$p_{disc}$","sens","FPR")]   # effect of increase in beta
table_FS = table1[rep(c(25:28,49:52),each=2)+rep(c(0,96),times=8),c("$\\beta$","$\\phi$","n","$p_{disc}$","sens","$sens_{pre}$","FPR")]
tab2=xtable(table_FS[table_FS$n==200,-3],
            digits=3,
            table.placement="!h",
            caption="Sensitivity and FPR both generally improve as LFC increases, especially in noisier datasets.",
            label = "tab:2",na.print="")
print(tab2,include.rownames=F, sanitize.text.function=identity)
@
$K=2, n=200, LFC=1$ \\
Gene discovery performance highly dependent on pre-filtering performance.
\end{frame}

\begin{frame}{Gene discovery = better with higher $n$}
<<xtable3, results=tex, echo=FALSE>>=
# table1[rep(1:96,each=2)+rep(c(0,96),times=96),c("$\\beta$","$\\phi$","n","$p_{disc}$","sens","FPR")]   # effect of increase in beta
tab3=xtable(table_FS[table_FS$'$\\beta$'==6.5,-1],
            digits=3,
            table.placement="!h",
            caption="Sensitivity and FPR both generally improve as n increases.",
            label = "tab:3",na.print="")
print(tab3,include.rownames=F, sanitize.text.function=identity)
@
$\beta=6.5, K=2, LFC=1$
\end{frame}

\begin{frame}{Increasing $\phi$ decreases clustering and gene selection performance}
<<xtable5, results=tex, echo=FALSE>>=
tablephi1=table1[rep(1:24,each=4)+rep(c(0,24,48,72),times=24),c("$\\beta$","$\\phi$","LFC","n","K","$K_{FSC}$","$OA_{FSC}$","$ARI_{FSC}$","$p_{disc}$","sens","FPR","$g_{disc}$")]
tablephi2=table1[rep(1:24,each=4)+rep(c(0,24,48,72)+96,times=24),c("$\\beta$","$\\phi$","LFC","n","K","$K_{FSC}$","$OA_{FSC}$","$ARI_{FSC}$","$p_{disc}$","sens","FPR","$g_{disc}$")]
tab5=xtable(tablephi1[37:40,c("$\\phi$","$K_{FSC}$","$OA_{FSC}$","$ARI_{FSC}$","sens","FPR","$g_{disc}$")],
            digits=3,
            table.placement="!h",
            caption="Effect of increased simulated noise on gene discovery. Performance decreases, as it is harder to distinguish differences between clusters in noisier datasets",
            label = "tab:5",na.print="")
print(tab5,include.rownames=F, sanitize.text.function=identity)
@
$\beta=3.75, K=4, n=200, p_{disc}=0.05, LFC=1$
\end{frame}

\begin{frame}{Order Selection = very important!}
Correlation between OA and ARI:\\
<<results=tex,echo=FALSE>>=
cor(table1$'$ARI_{FSC}$',table1$'$OA_{FSC}$')
@
\\
\end{frame}

\begin{frame}{Cluster Prediction Performance}     % Prediction accuracy was 100% for correctly selected orders, no matter what other params?!
\begin{itemize}
\item Test set: $0.1n$ simulated, based on same parameters as training set
\item Predictions performed just on simulation runs that correctly selected the order
\item Clustering on newly simulated samples, based on fitting parameter estimates
  \begin{itemize}
  \item Average of $99.9\%$ accuracy of subtype prediction (across all conditions) when order was correctly selected
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Order Selection of Other Methods}
\small\addtolength{\tabcolsep}{-3pt}
<<xtable8, results=tex, echo=FALSE>>=
tab8 = xtable(table1[c(61:64),c("LFC","n","$K_{FSC}$","$K_{iCl}$","$K_{HC}$","$K_{KM}$","$K_{NBMB}$","$K_{lMC}$","$K_{vMC}$","$K_{rMC}$")],
            digits=3,
            table.placement="!h",
            caption="Select examples of order selected by iCluster+ (iCl), hierarchical clustering (HC), K-medoids (KM), NB.MClust (NBMB), and mclust performed on log, variance-stabilizing, and rlog transformations (lMC, vMC, and rMC, respectively). Note: iCl, KM, and NBMB did not have an option to search K=1.",
            label = "tab:8",na.print="")
print(tab8,include.rownames=F, sanitize.text.function=identity)
@
$\beta=6.50, \phi=1.00, K=4$
\end{frame}

\begin{frame}{Clustering Performance of Other Methods}
\small\addtolength{\tabcolsep}{-4pt}
<<xtable9, results=tex, echo=FALSE>>=
tab9 = xtable(table1[c(61:64),c("LFC","n","$ARI_{FSC}$","$ARI_{iCl}$","$ARI_{HC}$","$ARI_{KM}$","$ARI_{NBMB}$","$ARI_{lMC}$","$ARI_{vMC}$","$ARI_{rMC}$")],
            digits=3,
            table.placement="!h",
            caption="Select examples of clustering performance by competing methods. At this high level of noise, iCl, HC, and KM on average significantly underselected $K=2$. Overall, out of the transformed mclust runs, lMC seems to perform best. FSCseq outperforms all other methods across the board.",
            label = "tab:9",na.print="")
print(tab9,include.rownames=F, sanitize.text.function=identity)
@
$\beta=6.50, \phi=1.00, K=4$
\end{frame}

\begin{frame}
\begin{figure}
\begin{center}
\includegraphics[width=120mm]{C:/Users/limdd/Documents/Research/Sweave/Project1/select_sim_bplots_ARI.png}
\end{center}
\caption{"Boxplots of ARI at fixed case of $K=6$, $\beta=6.5$, $LFC=1$, $n=100$, and $p_{disc}=0.05$ of competing methods at low ($\phi=0.15$, top-left), moderate ($\phi=0.35$, top-right), moderately high ($\phi=0.50$, bottom-left), and very high ($\phi=1.00$, bottom-right) noise.}
\label{fig:bplots2}
\end{figure}
\end{frame}
%%% 4. Real Data %%%
\section{Real Data}

\begin{frame}{TCGA BRCA}
\begin{itemize}
\item Excluded Normal-like ($n=3$) and Her2 ($n=6$) subtypes
\item Pre-filtered low purity ($purity < 0.9$) samples
\item Pre-filtered low count genes, and selected top 50\% MAD genes
  \begin{itemize}
  \item 47 of 50 PAM50 genes were in the top 50\% MAD
  \end{itemize}
\item 3 subtypes, $n=123$, $g=9062$
\item Clinical survival: Nonmissing survival times from 21 of 123 subjects.
  \begin{itemize}
  \item Not many survival endpoints to analyze.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Permutation-based Survival}
\begin{itemize}
\item Log-rank test based on Chi-square approximation widely used
\item Approximation may not be appropriate (Rappoport 2018)
  \begin{itemize}
  \item Log-rank p-values based on empirical and Chi-square tests differed greatly
  \item Empirical p-value can overcome bias when small #samples per cluster
  \item Approximate p-values were always more significant
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{TCGA BRCA Results}
<<xtable10,results=tex,echo=F>>=
#load("C:/Users/limdd/Documents/Research/Sweave/Presentation102618/Results/BRCA_table.RData")
load("C:/Users/limdd/Documents/Research/Real Data/TCGA BRCA/BRCA_compare_res.RData")
BRCA_tab = summary_table[,1:6]      # Exclude Hazards Ratios 
BRCA_tab = BRCA_tab[,-3]            # exclude silhouette values
tab10 = xtable(BRCA_tab,digits=3,caption="Cluster analyses and clinical analyses in clustering derived by FSCseq and competing methods, compared to the annotated cluster labels (anno).",
              label="tab:10",table.placement="H",na.print="",
              floating.environment = "table")
print(tab10,sanitize.text.function=identity,table.placement="H")
@
\end{frame}

\begin{frame}{TCGA BLCA}
\begin{itemize}
\item Excluded Luminal-infiltrated ($n=1$) and Luminal ($n=4$) subtypes
\item Pre-filtered low purity ($purity < 0.8$) samples
\item Pre-filtered low count genes, and selected top 50\% MAD genes
\item 3 subtypes, $n=99$, $g=8874$
\item Clinical survival: Nonmissing survival times from 29 of 99 subjects.
\end{itemize}
\end{frame}

\begin{frame}{TCGA BLCA Results}
<<xtable11,results=tex,echo=F>>=
#load("C:/Users/limdd/Documents/Research/Sweave/Presentation102618/Results/BLCA_table.RData")
load("C:/Users/limdd/Documents/Research/Real Data/TCGA BLCA/BLCA_compare_res.RData")
BLCA_tab = summary_table[,1:6]      # Exclude Hazards Ratios
BLCA_tab = BLCA_tab[,-3]            # Exclude silhouette values
tab11 = xtable(BLCA_tab,digits=3,caption="Cluster analyses and clinical analyses in clustering derived by FSCseq and competing methods, compared to the annotated cluster labels (anno).",
              label="tab:11",table.placement="H",na.print="",
              floating.environment = "table")
print(tab11,sanitize.text.function=identity,table.placement="H")
@
\end{frame}

%%% 5. Discussion %%%
\section{Discussion}
\begin{frame}{Conclusions}
\begin{itemize}
\item FSCseq outperforms competitors in clustering RNAseq in simulations, and challenges methods already used in real data
\item Gene discovery performance highly dependent on cluster separation (LFC)
\item Order selection problem difficult but very significant
\item As number of samples ($n$) increases, clustering performance will increase
\item Chi-square approximation Log-rank test used to measure efficacy of clustering needs to be questioned
\end{itemize}
\end{frame}

\begin{frame}{Future Directions}
\begin{itemize}
\item Pre-filtering by MAD can be limiting factor in gene discovery
\item Adjust for purity and other known biases
\item Incorporate gene-to-gene correlation
\item Extend to integrative analysis of multiple omics
\end{itemize}
\end{frame}

\section*{Acknowledgments}
\begin{frame}{Special Thanks}
Dr. Naim Rashid, Dr. Joseph Ibrahim
\end{frame}

\end{document}