\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
%\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{float}
\usepackage{hyperref}
\usepackage[noabbrev,capitalize]{cleveref}
%\usepackage{biblatex}
%\addbibresource{C:/Users/limdd/Documents/Research/Sweave/Project1/sample.bib}


%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%

\begin{document}
\SweaveOpts{concordance=TRUE}


\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\if1\blind
{
  \title{\bf Mixture Negative Binomial Expectation Maximization (NB-EM) Algorithm for Unsupervised Clustering}
  \author{David Lim\thanks{
    The authors gratefully acknowledge \textit{please remember to list all relevant funding sources in the unblinded version}}\hspace{.2cm}\\
    Department of Biostatistics, UNC, Chapel Hill\\
    Naim Rashid \\
    Department of Biostatistics, UNC, Chapel Hill \\
    Joseph Ibrahim \\
    Department of Biostatistics, UNC, Chapel Hill}

  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Mixture Negative Binomial Expectation Maximization (NB-EM) Algorithm for Unsupervised Clustering}
\end{center}
  \medskip
} \fi

\bigskip

%%%%%%%%%%%%%%%%%%%%%%%
%%%%% 0. ABSTRACT %%%%%
%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
Clustering is a form of unsupervised learning that aims to uncover latent groups within data based on similarity across a set of features. A common application of this in biomedical research is in deriving novel cancer subtypes from patient gene expression data, given a set of informative genes. However, it is typically unknown a priori what genes may be informative in discriminating between clusters, and what the optimal number of clusters is. Few methods exist for unsupervised clustering of RNA-seq data that can simultaneously adjust for between-sample normalization factors, account for effects of potential confounding variables, and cluster patients while selecting cluster-discriminatory genes. To address this issue, we propose a mixture model EM algorithm with a group truncated lasso penalty. The maximization is done by coordinate-wise descent using the IRLS algorithm, allowing us to include normalization factors and predictors into our modeling framework. The EM framework allows for subtype prediction in new patients via posterior probabilities of cluster membership given the fitted model. Based on simulations and real data, we show the utility of our method.
\end{abstract}

\noindent%
{\it Keywords:}  unsupervised clustering, genomics, EM
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% 1. INTRODUCTION %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

Introduction
Body of paper.  Margins in this document are roughly 0.75 inches all
around, letter size paper.


%%%%%%%%%%%%%%%%%%%%%%
%%%%% 2. METHODS %%%%%
%%%%%%%%%%%%%%%%%%%%%%

\section{Methods}
\label{sec:meth}

Our novel method utilizes the widely known Expectation-Maximization algorithm. Most model-based methods dealing with this type of data typically proceed with the assumption of an underlying Poisson distribution on the dataset. However, there is much evidence to suggest that the Poisson assumption is too conservative for this type of genomic data. Thus, we proceed with the more liberal assumption of a negative binomial distribution, with a gene-wise(?) overdispersion parameter that accounts for the extra-Poisson variation.

Our main goal is two-fold: 1. to determine discriminatory genes based on differential expression of RNA-seq data, and 2. to cluster subjects based on similar expression profiles of genes. Also, we believe that our method is special in that it may be used to derive posterior probabilities of a sample's subtype based on the model that is fit on the dataset, which may be especially useful in prediction.

%%% 2.1 Obj fx %%%
\subsection{Likelihood Model}
The objective function is the well-known Q function, which is the conditional expectation of the complete data log-likelihood function, which is given by 

\begin{equation}
log[L(\Psi)]=\sum_{i=1}^{n} \sum_{k=1}^{K} z_{ik} \{log(\pi_k)+log[f_k(\boldsymbol{y_i}; \boldsymbol{\beta_k},\boldsymbol{\phi_k})]\}+p_\lambda(\beta)
\end{equation}

where $z_{ik}=I(z_i=k)$ denotes the indicator of subject $i$ being in cluster $k$ and $f_k(y_i; \beta_k)=\prod_{j=1}^{g} f_{jk}(y_{ij}; \beta_{jk},\phi_{jk})$. Given $y_{ij}$ is the count of the i'th subject for the j'th gene, where $i=1,...,n$ and $j=1,...,g$. The cluster means are given by $\beta_{jk}$ where $k=1,...,K$. The cluster proportions are given by $\pi_k$. The underlying assumption here is that the expression of each gene is uncorrelated to that of any other gene.

Because $z_{ik}$ is unobservable, we estimate this quantity by the conditional expectation of $z_{ik}$ and the parameters estimated from the $m$th step. In this algorithm, we implement gene-by-gene iteratively reweighted least squares method, using coordinate-wise descent along the clusters. We include an elastic net penalty, which shrinks the means of each cluster closer together and selects genes by thresholding the differences to 0.


% 2.1.1 Penalty
\subsubsection{Penalty}
The penalty is the so-called grouped truncated lasso, which was introduced in Pan et al (2013).
\begin{equation}
p_{\lambda}(\beta)=\frac{\lambda_1}{2}\sum_{k<l} \norm{\beta_k-\beta_l-\theta_{kl}}_2^2 + \lambda_2 \sum_{k<l}TLP(\norm{\theta_{kl}}_2;\tau)
\end{equation}

where $\theta_{kl}=\beta_k-\beta_l$ is a reparametrization to store the difference between cluster means in the previous iteration, and $TLP(\alpha;\tau)=min(\abs{\alpha},\tau)$. In this penalization scheme, $\lambda_1$ is the parameter that controls shrinkage of the cluster means towards one another, and $\lambda_2$ is the parameter that controls selection by thresholding two cluster means to be equal if they are close enough together. The $\tau$ parameter is a threshold that introduces no further penalty when the difference between cluster means is sufficiently large.

%%% 2.2 Computation %%%
\subsection{Computation}
% 2.2.1 E Step
\subsubsection{E Step}
We calculate the conditional expectation of the $z_{ik}$ given the current estimates of the parameters. We denote $\hat{z}_{ik}^{(m)} = E[z_{ik} \mid \mathbf{y}, \boldsymbol{\hat{\beta}^{(m)}}, \boldsymbol{\hat{\pi}^{(m)}}]$. Another way to think about this quantity is as the posterior probability of subject $i$ being in cluster $k$. These quantities are passed through as weights in the estimation of the coefficients in the maximization step. The update on these weights are:

\begin{equation}
\hat{z}_{ik}^{(m)}=\dfrac{\hat{\pi}_k^{(m)}f_k(\boldsymbol{y_i};\boldsymbol{\hat{\beta}_k^{(m)},\hat{\phi}_k^{(m)}})}{\sum_{l=1}^{K}\hat{\pi}_l^{(m)}f_l(\boldsymbol{y_i};\boldsymbol{\hat{\beta}_l^{(m)},\hat{\phi}_l^{(m)}})}
\end{equation}

There are some variants of the E step that have been proposed by Biernacki et al (2003). Replacing the E step update with classification and/or stochastic E step updates has been shown to help the EM algorithm to not get stuck at a local maxima. In our numerical analyses, we will explore the effects of using these alternate E step updates on performance.


% 2.2.2 M Step
\subsubsection{M Step}
In the M step of the EM algorithm, we update the current estimates of the parameters to maximize the penalized objective function. The maximization of $\pi_k$ and $\beta_{jk}$ are separable, thus they can be done independently.


%Est of beta
The maximization of the cluster means and dispersion parameter(s) is performed by iteratively reweighted least squares using a coordinate-wise descent algorithm. We first maximize the penalized objective function to estimate the cluster means. This is accomplished by using a transformed response, as implemented in Breheny et al (2011) (ncvreg paper). We fix a gene j, and transform the responses by the following:

$$\tilde{y}_{ik}=\hat{\eta_k}+(\frac{y_i-g(\hat{\eta}_k)}{g'(\hat{\eta}_k)})$$

Here, $g()$ refers to the inverse link function for the Negative Binomial family with log link, thus $g(\eta)=\mu$, where $g^{-1}(\mu)=log(\mu)$ is the standard log link function.

Using this transformation, the penalized objective function (Breheny 2011) becomes:

\begin{equation}
Q_j(\boldsymbol{\beta}) \approx \frac{1}{2n}(\mathbf{\tilde{y}}-\mathbf{X}\boldsymbol{\beta})'\mathbf{W}(\mathbf{\tilde{y}}-\mathbf{X}\boldsymbol{\beta})+\sum_{k=1}^{K}p_{\boldsymbol{\lambda}}(\beta_{jk})
\end{equation}

where X is an $(nK)$x$K$ matrix of 1's and 0's that represent the indicator of each cluster for each transformed response, and W is the matrix of the E step weights. In this framework, we pass on $\hat{z}^{(m)}_{ik}$ as the weights in regressing $\tilde{y}_{ik}$ on X.

Therefore, we have $w_{ik}=\sqrt{\dfrac{\hat{z}^{(m)}_{ik} g'(\eta)^2}{V(\mu)}}$, where the variance is $V(\mu)=\mu + \phi\mu^2$ for the Negative Binomial. Equation 3 is maximized for each value of $j=1,...,g$. The update equations become:

\begin{equation}
\begin{split}
\hat{\beta}_k^{(m+1)}&=\dfrac{\frac{1}{n}\sum_{i=1}^{n}w_{ik}\tilde{y}_{ik}+\lambda_1[\sum_{l>k}(\hat{\beta}_l^{(m)}+\hat{\theta}_{kl}^{(m)})+\sum_{l<k}(\hat{\beta}_l^{(m+1)}-\hat{\theta}_{lk}^{(m)})]}{\lambda_1(K-1)+\frac{1}{n}\sum_{i=1}^{n}w_{ik}} \\
\hat{\theta}_{kl}^{(m+1)}&=\begin{cases} 
      \hat{\beta}_k^{(m+1)}-\hat{\beta}_l^{(m+1)} & \norm*{\hat{\theta}_{kl}^{(m)}}_2 \geq \tau \\
      ST(\hat{\beta}_k^{(m+1)}-\hat{\beta}_l^{(m+1)},\lambda_2) & otherwise
   \end{cases}
\end{split}
\end{equation}

with $ST(\alpha,\lambda)=sign(\alpha)(\lvert \alpha \rvert-\lambda)_+$ denoting the usual soft-thresholding rule. The gene $j$ is fixed, and the above procedure is repeated for every gene $j=1, ..., g$. 

In this framework, $\hat{\theta}$ represents shrinkage on the cluster means, with the cluster means set as equal when they are within $\lambda_2$ from each other.



%Est of phi
After sequentially updating the cluster means, we estimate the overdispersion parameter $\phi$ using a maximum likelihood (ML) approach. However, this approach is limited due to instability when sample size is small. To mitigate this, we include a very small penalty (order of $10^{-50}$) on the ML estimation of $\phi$ to stabilize the estimate in the low sample setting.

We also compared the effects of using gene-specific and cluster-specific dispersion parameters. It is not known which would be more appropriate for this type of data. Cluster-specific dispersion parameters may provide a better fitting model to the data, but gene-specific dispersion parameters will allow for more sparsity and avoid the issue of overfitting. The function in the R package will include the option to use either scheme.


% 2.2.3 Convergence
\subsubsection{Stopping Criteria}
The stopping criterion for the EM algorithm is based on a threshold on the Q function. The algorithm is considered to have "converged" if $\lvert Q^{(m+1)}-Q^{(m)} \rvert< \epsilon_1$

The stopping criterion for the IRLS in the M step is based on a threshold on the sum of squares of the parameters. The algorithm is considered to have converged if $\lvert \boldsymbol{\beta}^{(l+1)}-\boldsymbol{\beta}^{(l)} \rvert + \lvert \boldsymbol{\phi}^{(l+1)}-\boldsymbol{\phi}^{(l)} \rvert < \epsilon_2$

We set both $\epsilon_1 = 10^{-6}$ and $\epsilon_2 = 10^{-6}$.

% 2.2.4 Tuning Parameters
\subsubsection{Tuning Parameters}

The optimal number of clusters K is found by searching over a range of values $K=(1, ...,7)$ and comparing the fit using the yielded BIC values. \cref{fig:1} gives a graphical representation of the procedure on an example simulated dataset

\begin{figure}
\begin{center}
<<echo=FALSE,fig=TRUE>>=
load("C:/Users/limdd/Documents/Research/Sweave/Project1/list_BIC_n100_g865_K3.Rout")
plot(list_BIC,ylab="BIC",xlab="K",main="Plot of BIC vs. order")
@
\end{center}
\caption{Order selection is done by choosing the order (K) that minimizes the BIC. Here, the true order is K = 3 in a simulated dataset of n = 100 and g = 835 (after pre-filtering low count genes), and 20 \% discriminatory genes with log fold change of 2.}
\label{fig:1}
\end{figure}

Optimal tuning parameters for \(\lambda_2\) and \(\tau\) are also found by using the BIC criterion. The algorithm searches over a grid of values, and selects the combination that yields the lowest BIC. Pan proposed fixing the shrinkage parameter $\lambda_1=1$, varying the thresholding parameters $\lambda_2$ and $\tau$. We set $\lambda_1=1$ and search over $\lambda_2=(0.01, 0.05, 0.1, 0.15, 0.2)$ and $\tau=(0.1, 0.3, 0.5, 0.7, 0.9)$. Pan further proposed re-running the algorithm after doubling the value of $\lambda_1$ upon convergence in order to prevent the algorithm from being stuck at local maxima. For the sake of computation time, we instead implemented the classification and stochastic E step weights as an alternative solution to this issue.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% 3. Numerical Examples %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Numerical Examples}
\label{sec:examples}

%%% 3.1 Simulations %%%
\subsection{Simulations}

It is well-known that a majority of the genes in a typical dataset is nondiscriminatory across subtypes. In order to cluster subtypes accurately, it is important to filter out genes that are obviously nondiscriminatory in nature. One common way to do this is by filtering out genes whose median absolute deviation is low (Chung et al 2008). We utilize this, along with a subsequent wrapper in our method to perform pre-filtering and variable selection on the data for analysis (Ritter et al book).

(MAYBE ADD: NB GOF pvalue vs MAD)

We simulated 100 datasets per condition, and spanned an extensive set of conditions. Variable conditions included log fold change ($LFC$), number of clusters ($K$), number of samples per cluster ($n_k$), number of genes ($g$), and proportion of discriminatory genes ($p_{disc}$). We searched over $LFC=(0.5, 1, 2)$, $K=(2, 3, ..., 7)$, $n_k=(25, 50)$, $g=(1000, 2000)$, and $p_{disc}=(0.05, 0.1)$

To explore the behaviors in various realistic settings, we simulated low, medium, and high expression and noise values based on the 25th, 50th, and 75th quantiles of the estimates of expression and dispersion in the TCGA Breast Cancer dataset. These values were approximately $\beta=(3.75,6.5,7.85)$ and $\phi=(0.15,0.35,1.00)$, respectively.

After simulating, we imposed pre-filtering and kept just the genes with the top 25\% of MAD. \cref{tab:1} shows the performance statistics with varying coefficients and noise, averaged across all of the other variables ($LFC$, $K$, $n_k$, $g$, and $p_{disc}$). One very promising added utility of our method is the ability to perform prediction based on the estimated parameters and the posterior weights from the E step. We analyze the performance of clustering, discovering discriminatory genes across clusters, as well as predicting on newly simulated samples (floor(0.1*n)) in \cref{tab:1}

<<xtable1, results=tex, echo=FALSE>>==
library(xtable)
load("C:/Users/limdd/Documents/Research/Sweave/Project1/simulation_res.RData")
colnames(summary_tab_coefphi)[1:3] = c("$\\beta$","$\\phi$","Order Acc")
tab1 = xtable(summary_tab_coefphi,
              digits=2,
              table.placement="!h",
              caption="Effects of varying coefficients ($\\beta$) and noise ($\\phi$) in simulated datasets on performance",
              label = "tab:1")      # K=3, n=50, g=1000
print(tab1,include.rownames=F, sanitize.text.function=identity)
@

As the average expression of the genes ($\beta$) goes up, we see that clustering performance (ARI) and order selection accuracy both increase. This is because as the mean expression for a gene increases, the distance two clusters in that particular gene increases. since the log fold change creates a greater separation at higher expressions. For example, the distance between $\beta=3$ and $\beta=4$ is much smaller than the distance between $\beta=6$ and $\beta=7$, since $\mu=e^{\beta}$, and $e^4-e^3 < e^7-e^6$.

Conversely, as noise ($\phi$) increases, we see a decrease in clustering and order selection accuracy. The noisier the dataset, the harder it will be to distinguish whether the observed differential expression has biological significance, or is simply due to randomness in  the data. 

Another useful feature of our algorithm is the ability to perform simultaneous variable selection while clustering. This helps us uncover which genes are discriminatory across clusters. We note that the sensitivity, or the proportion of discriminatory genes that are determined correctly to be discriminatory, is very good when the noise is small, but gets worse as noise is increased. We found that the false positive rate (FPR), or the rate at which nondiscriminatory genes are falsely determined to be discriminatory, does not follow this trend, but remains relatively constant through varying noise. 

A common integrative clustering method that is able to handle count data is iClusterPlus (Mo et al 2012). It assumes a Poisson distribution on the data, and uses a L1 penalized likelihood model to estimate the parameters, while using a sampling scheme from the posterior distribution of the latent variables to discover clustering labels. We compared our method to just the count data portion of iClusterPlus's integrative method.

Also, Two other clustering methods that have been found to be very useful for clustering RNA-seq count data are average-linkage hierarchical clustering and K-medoids (Jaskowiak et al 2018). There has been a study that suggests that these were among the most robust and efficient in clustering this type of data. (MAYBE INSERT THEORY OF HC AND K-MEDOIDS CLUSTERING).

(((INSERT TABLE 2: comparison between EM, iCluster+, HC, K-Medoids. running now)))

% <<xtable2, results=tex,echo=FALSE>>==
% @

We analyzed some very natural variants of our method. For one, we have so far restricted the simulations to contain gene-specific dispersions. It is not currently known whether a scheme with one dispersion parameter across all clusters for each gene adequately portrays real data. It may sometimes be more appropriate to introduce cluster-specific dispersion parameters to better fit the data, yielding more accurate estimates. However, this may also result in overfitting the data, which would cause our model to get stuck at a local maxima. One potential drawback that may result is that the order may be underestimated, as the extra dispersion parameters may cause the algorithm to favor a smaller number of clusters in the data.

Secondly, research has shown that the EM algorithm tends to converge to the local maximum, rather than the global maximum. Thus, this may result in the algorithm being "stuck" at less than ideal conditions. One way to prevent this is to replace the current E step with the classification E step (Biernacki et al 2003). This variant of the E step introduces some randomness by drawing the posterior probabilities closer together, causing the algorithm to overcome local maxima through small perturbations of the clustering index. \cref{tab:3} \cref{tab:4} show the results of these variants under simulated gene-specific and cluster-specific dispersions, respectively, as well as the analogous iCluster+ results. Results are based on 10 simulated datasets for each case. (1) gEM is the EM with gene-specific dispersions, as before; (2) gCEM is the CEM with gene-specific dispersions; (3) clEM is the EM with cluster-specific dispersions; and (4) clCEM is the CEM with cluster-specific dispersions.

%%%%% EXCLUDE ICLUSTER+ COMPARISON FOR TABLES 3 AND 4. JUST COMMENT ON IT IN PAPER %%%%%%%%%%%%%
<<xtable3,results=tex,echo=FALSE>>==
load("C:/Users/limdd/Documents/Research/Simulations/g_v_cl__EM_v_CEM/gvcl_EMvCEM.RData")
colnames(tab_g) = c("$K$","$p_{disc}$","EM ARI","Sensitivity","FPR","iClust $K$","iClust ARI","Pred Acc")
tab3 = xtable(tab_g[,c(2,1,6,3,7,4,5,8)],
              digits=2,
              table.placement="!h",
              caption="Clustering, discovery, and prediction performance of variants of EM. The simulated dispersions were gene-specific.",
              label = "tab:3")      # K=3, n=50, g=1000jo
print(tab3,include.rownames=T, sanitize.text.function=identity)

@

<<xtable4,results=tex,echo=FALSE>>==
#load("C:/Users/limdd/Documents/Research/Simulations/g_v_cl__EM_v_CEM/gvcl_EMvCEM.RData")
colnames(tab_cl) = c("$K$","$p_{disc}$","EM ARI","Sensitivity","FPR","iClust $K$","iClust ARI","Pred Acc")
tab4 = xtable(tab_cl[,c(2,1,6,3,7,4,5,8)],
              digits=2,
              table.placement="!h",
              caption="Clustering, discovery, and prediction performance of variants of EM. The simulated dispersions were cluster-specific.",
              label = "tab:4")      # K=3, n=50, g=1000
print(tab4,include.rownames=T, sanitize.text.function=identity)
@

From \cref{tab:3} and \cref{tab:4}, we see that iCluster+ seems to overestimate the order in each case. Because it treats count data as distributed Poisson, it is likely that iCluster+ is not able to account for the extra-Poisson variation caused by the simulated noise. Also, in general, the order is underestimated for the EM runs with cluster-specific dispersion parameters than the runs with gene-specific dispersion parameters. This is due to the fact that allowing for cluster-specific dispersions overfits the data, so the cluster-specific dispersions is sufficient to explain the unaccounted clusters in the results. Because of this, the clustering accuracy suffers.

We also notice the effect of the classification E step modification in our algorithm. We see that for a gene-specific dispersion scheme, the CEM seems to increase the ARI slightly. We notice that in \cref{tab:3}, the CEM caused the cluster-specific dispersion EM model to select the incorrect order. This portrays a risk of the CEM, as the variant E step adds randomness by drawing posterior probabilities closer together, and may lead to such instabilities.

Although the clustering accuracy was dependent on the dispersion scheme, the sensitivity and false positive rate in discovering discriminatory genes seem to not be affected. In each case, the algorithm was able to correctly distinguish all of the simulated discriminatory genes, with very few false positive cases.

The prediction accuracy was calculated by first inputting the correct order in cases that the order was incorrectly selected, simulating 20 samples ($n_{pred}=0.1*n$) with the same simulated parameters, and selecting the cluster with the highest posterior probability for each sample. In either intrinsic dispersion scheme, we see that the prediction accuracy is very high.

%%% 3.2 Real Data %%%
\subsection{Real Data}

\subsubsection{TCGA Breast Cancer Dataset}

We performed our clustering analysis on The Cancer Genome Atlas breast cancer dataset. This dataset contained 1215 subjects and 19682 genes. Of these subjects, we narrowed down to subjects who had prior annotated subtypes in the dataset based on the PAM50 genes. We also dropped the normal-like subtype, which occurred in just 8 cases out of 521 subjects. Thus, we analyzed 513 samples with 4 different subtypes: 231 luminal A, 127 luminal B, 97 basal-like, and 58 HER2-enriched. Then, we filtered out genes with low count by taking excluding from analysis the genes with MAD scores below the 75th quantile, yielding 4921 total genes. (ADD ARI and SILHOUETTE DERIVED for CEM, and rerun for current EM)

We compared our method to the count data method of iCluster+ (Mo et al 2012). We used the same criterion (BIC) to search for the optimal order and tuning parameters. As shown in \cref{tab:5}, our method found the correct order of 4, while iCluster+ favored a higher number of clusters. Our method also yielded a greater agreement (ARI) to the annotated clusters, as well as a higher mean silhouette value. (SHOULD I INCLUDE EQUATIONS FOR ARI/silhouette here? Ge et al 2017 provides equation for silhouette)

<<xtable5,results=tex,echo=F>>=
library(mclust)
library(cluster)
load("C:/Users/limdd/Documents/Research/BRCA_env.Rdata")
load("C:/Users/limdd/Documents/Research/X2.out")
true_K = length(levels(droplevels(as.factor(BRCA_subtypes))))
EM_K = X2$EM_maxK
EM_ARI=adjustedRandIndex(X2$EM_clusters,BRCA_subtypes)
i_K = X2$i_maxK
i_ARI=adjustedRandIndex(X2$i_clusters,BRCA_subtypes)

# Silhouette
load("C:/Users/limdd/Documents/Research/Sweave/Project1/BRCA_dist.out")
i_sil = silhouette(X2$i_clusters,d)
EM_sil = silhouette(X2$EM_clusters,d)

EMclust = c(EM_K,EM_ARI,mean(EM_sil[,3]))
iclust = c(i_K,i_ARI,mean(i_sil[,3]))


tab5 = data.frame(rbind(round(EMclust,3),round(iclust,3)))
colnames(tab5) = c("K","ARI","Silhouette")
rownames(tab5) = c("EM","iCluster+")

tab5 = xtable(tab5,caption="Comparing iCluster+ with EM method. True (annotated) order is 4. Values of ARI closer to 1 indicate better clustering results with respect to the true (annotated) clusters. Silhouette values cluster to 1 indicate better clustering results based on dissimilarity matrix on normalized counts.",label="tab:5")
print(tab5)

#aggregate(EM_sil[,3], list(EM_sil[,1]), mean)   # See silhouette value means per cluster
#aggregate(i_sil[,3], list(i_sil[,1]), mean)

@

It it known that the PAM50 genes are genes of significant interest for breast cancer. We performed further analysis on just these PAM50 genes. \cref{fig:2} is a heatmap of the known PAM50 genes, ordered by the clustering results of our CEM, and using the scheme with gene-specific dispersion parameters. (NOTE TO SELF: with initial Tau = 2. need to be re-run and made bigger to see gene labels)

\begin{figure}[H]
\begin{center}
\includegraphics{C:/Users/limdd/Documents/Research/Sweave/Project1/BRCA_pam50_CEM.png}
\end{center}
\caption{Heatmap of TCGA Breast Cancer PAM50 genes, comparing annotated clusters with CEM and EM algorithms.}
\label{fig:2}
\end{figure}


Using just these genes, we found $ARI = 0.407$ using the cluster-specific dispersion parameters, and $ARI = 0.523$ using the gene-specific dispersion parameters.

\subsubsection{TCGA Bladder Cancer Dataset}

We also performed similar analyses on the TCGA Bladder Cancer dataset. This dataset contained 408 samples with annotated subtypes: 142 basal-squamous, 142 luminal-papillary, 78 luminal-infiltrated, 26 luminal, and 20 neuronal. As before, we filtered just the genes with the top 25 quantile MAD scores, leaving 4916 genes in the analysis.



%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% 4. DISCUSSION %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
\label{sec:disc}

%\cite{Campbell02}, \cite{Schubert13}, \cite{Chi81}

\bigskip
\begin{center}
{\large\bf SUPPLEMENTARY MATERIAL}
\end{center}

\begin{description}

\item[Title:] Brief description. (file type)

\item[R-package for  MYNEW routine:] R-package ÒMYNEWÓ containing code to perform the diagnostic methods described in the article. The package also contains all datasets used as examples in the article. (GNU zipped tar file)

\item[HIV data set:] Data set used in the illustration of MYNEW method in Section~ 3.2. (.txt file)

\end{description}

\section{References}

%\bibliography{C:/Users/limdd/Documents/Research/Sweave/Project1/sample.bib}
%\bibliographystyle{agsm}
%\printbibliography


\end{document}