%  template.tex for Biometrics papers
%
%  This file provides a template for Biometrics authors.  Use this
%  template as the starting point for creating your manuscript document.
%  See the file biomsample.tex for an example of a full-blown manuscript.

%  ALWAYS USE THE referee OPTION WITH PAPERS SUBMITTED TO BIOMETRICS!!!
%  You can see what your paper would look like typeset by removing
%  the referee option.  Because the typeset version will be in two
%  columns, however, some of your equations may be too long. DO NOT
%  use the \longequation option discussed in the user guide!!!  This option
%  is reserved ONLY for equations that are impossible to split across 
%  multiple lines; e.g., a very wide matrix.  Instead, type your equations 
%  so that they stay in one column and are split across several lines, 
%  as are almost all equations in the journal.  Use a recent version of the
%  journal as a guide. 
%  
%\documentclass[12pt]{article}
\documentclass[useAMS,usenatbib,referee]{biom}
%documentclass[useAMS]{biom}
%
%  If your system does not have the AMS fonts version 2.0 installed, then
%  remove the useAMS option.
%
%  useAMS allows you to obtain upright Greek characters.
%  e.g. \umu, \upi etc.  See the section on "Upright Greek characters" in
%  this guide for further information.
%
%  If you are using AMS 2.0 fonts, bold math letters/symbols are available
%  at a larger range of sizes for NFSS release 1 and 2 (using \boldmath or
%  preferably \bmath).
% 
%  Other options are described in the user guide. Here are a few:
% 
%  -  If you use Patrick Daly's natbib  to cross-reference your 
%     bibliography entries, use the usenatbib option
%
%  -  If you use \includegraphics (graphicx package) for importing graphics
%     into your figures, use the usegraphicx option
% 
%  If you wish to typeset the paper in Times font (if you do not have the
%  PostScript Type 1 Computer Modern fonts you will need to do this to get
%  smoother fonts in a PDF file) then uncomment the next line
%  \usepackage{Times}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{xfrac}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{url} % not crucial - just used below for the URL
\usepackage{float}
\usepackage[T1]{fontenc}
\usepackage{colortbl}
\usepackage{rotating}

%\usepackage{biblatex}
%\usepackage{harvard}


%%%%%%%%%%%%%%%%%%%% MACROS %%%%%%%%%%%%%%%%%
\def\bSig\mathbf{\Sigma}
\newcommand{\VS}{V\&S}
\newcommand{\tr}{\mbox{tr}}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
%
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

%  The rotating package allows you to have tables displayed in landscape
%  mode.  The rotating package is NOT included in this distribution, but
%  can be obtained from the CTAN archive.  USE OF LANDSCAPE TABLES IS
%  STRONGLY DISCOURAGED -- create landscape tables only as a last resort if
%  you see no other way to display the information.  If you do do this,
%  then you need the following command.

%\usepackage[figuresright]{rotating}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%  Here, place your title and author information.  Note that in
%  use of the \author command, you create your own footnotes.  Follow
%  the examples below in creating your author and affiliation information.
%  Also consult a recent issue of the journal for examples of formatting.

\title[NTS:NEED TO ADD TCGA BRCA AND BLCA IN BIBLIO]{Model-based Simultaneous Unsupervised Clustering and Features Selection for Subtype Discovery of Raw RNA-seq Data}

%  Here are examples of different configurations of author/affiliation
%  displays.  According to the Biometrics style, in some instances,
%  the convention is to have superscript *, **, etc footnotes to indicate
%  which of multiple email addresses belong to which author.  In this case,
%  use the \email{ } command to produce the emails in the display.

%  In other cases, such as a single author or two authors from
%  different institutions, there should be no footnoting.  Here, use
%  the \emailx{ } command instead.

%  The examples below corrspond to almost every possible configuration
%  of authors and may be used as a guide.  For other configurations, consult
%  a recent issue of the the journal.

%  Single author -- USE \emailx{ } here so that no asterisk footnoting
%  for the email address will be produced.

%\author{John Author\emailx{email@address.edu} \\
%Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K.}

%  Two authors from the same institution, with both emails -- use
%  \email{ } here to produce the asterisk footnoting for each email address

%\author{John Author$^{*}$\email{author@address.edu} and
%Kathy Authoress$^{**}$\email{email2@address.edu} \\
%Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K.}

%  Exactly two authors from different institutions, with both emails
%  USE \emailx{ } here so that no asterisk footnoting for the email address
%  is produced.

% \author
% {John Author\emailx{author@address.edu} \\
% Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K.
% \and
% Kathy Author\emailx{anotherauthor@address.edu} \\
% Department of Biostatistics, University of North Carolina at Chapel Hill,
% Chapel Hill, North Carolina, U.S.A.}

%  Three or more authors from same institution with all emails displayed
%  and footnoted using asterisks -- use \email{ }

\author{David Lim$^*$\email{deelim@ad.unc.edu},
Naim Rashid$^{**}$\email{naim@unc.edu}, and
Joseph Ibrahim$^{***}$\email{ibrahim@bios.unc.edu} \\
Department of Biostatistics, University of North Carolina, Chapel Hill, NC, USA}

%  Three or more authors from same institution with one corresponding email
%  displayed

%\author{John Author$^*$\email{author@address.edu}, 
%Jane Author, and Dick Author \\
%Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K}

%  Three or more authors, with at least two different institutions,
%  more than one email displayed 

%\author{John Author$^{1,*}$\email{author@address.edu}, 
%Kathy Author$^{2,**}$\email{anotherauthor@address.edu}, and 
%Wilma Flinstone$^{3,***}$\email{wilma@bedrock.edu} \\
%$^{1}$Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K \\
%$^{2}$Department of Biostatistics, University of North Carolina at 
%Chapel Hill, Chapel Hill, North Carolina, U.S.A. \\
%$^{3}$Department of Geology, University of Bedrock, Bedrock, Kansas, U.S.A.}

%  Three or more authors with at least two different institutions and only
%  one email displayed

%\author{John Author$^{1,*}$\email{author@address.edu}, 
%Wilma Flinstone$^{2}$, and Barney Rubble$^{2}$ \\
%$^{1}$Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K \\
%$^{2}$Department of Geology, University of Bedrock, Bedrock, Kansas, U.S.A.}

\begin{document}
\SweaveOpts{concordance=TRUE}

% \setlength{\paperheight}{11in}
% \setlength{\paperwidth}{8.5in}


\date{{\it Received October} 2007. {\it Revised February} 2008.  {\it
Accepted March} 2008.}

%  These options will count the number of pages and provide volume
%  and date information in the upper left hand corner of the top of the 
%  first page as in published papers.  The \pagerange command will only
%  work if you place the command \label{firstpage} near the beginning
%  of the document and \label{lastpage} at the end of the document, as we
%  have done in this template.

%  Again, putting a volume number and date is for your own amusement and
%  has no bearing on what actually happens to your paper!  

\pagerange{\pageref{firstpage}--\pageref{lastpage}} 
\volume{64}
\pubyear{2008}
\artmonth{December}

%  The \doi command is where the DOI for your paper would be placed should it
%  be published.  Again, if you make one up and stick it here, it means 
%  nothing!

\doi{10.1111/j.1541-0420.2005.00454.x}

%  This label and the label ``lastpage'' are used by the \pagerange
%  command above to give the page range for the article.  You may have 
%  to process the document twice to get this to match up with what you 
%  expect.  When using the referee option, this will not count the pages
%  with tables and figures.  

\label{firstpage}

%  put the summary for your paper here

%% BLINDING %%
% \if1\blind
% {
%   \title{\bf Mixture Negative Binomial Expectation Maximization (NB-EM) Algorithm for Unsupervised Clustering}
%   \author{David Lim\thanks{
%     The authors gratefully acknowledge \textit{please remember to list all relevant funding sources in the unblinded version}}\hspace{.2cm}\\
%     Department of Biostatistics, UNC, Chapel Hill\\
%     Naim Rashid \\
%     Department of Biostatistics, UNC, Chapel Hill \\
%     Joseph Ibrahim \\
%     Department of Biostatistics, UNC, Chapel Hill}
% 
%   \maketitle
% } \fi
% 
% \if0\blind
% {
%   \bigskip
%   \bigskip
%   \bigskip
%   \begin{center}
%     {\LARGE\bf Mixture Negative Binomial Expectation Maximization (NB-EM) Algorithm for Unsupervised Clustering}
% \end{center}
%   \medskip
% } \fi

%%%%%%%%%%%%%%%%%%%%%%%
%%%%% 0. ABSTRACT %%%%%
%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
Clustering is a form of unsupervised learning that aims to uncover latent groups within data based on similarity across a set of features. A common application of this in biomedical research is in deriving novel cancer subtypes from patient gene expression data, given a set of informative genes. However, it is typically unknown a priori what genes may be informative in discriminating between clusters, and what the optimal number of clusters is. Few methods exist for unsupervised clustering of RNA-seq data that can simultaneously adjust for between-sample normalization factors, account for effects of potential confounding variables, and cluster patients while selecting cluster-discriminatory genes. To address this issue, we propose a model-based clustering algorithm that utilizes a finite mixture of regression (FMR) model with a hybrid L2 and SCAD penalty. The maximization is done by coordinate-wise descent using the IRLS algorithm, allowing us to include normalization factors, and potentially adjust for confounders in our modeling framework. Given the fitted model, our framework allows for subtype prediction in new patients via posterior probabilities of cluster membership. Based on simulations and real data, we show the utility of our method relative to competing approaches.
\end{abstract}

\begin{keywords}
unsupervised clustering, genomics, EM
\end{keywords}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% 1. INTRODUCTION %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
<<setup, include=FALSE, cache=FALSE, echo=FALSE>>=
setwd("C:/Users/limdd/Documents/Research/Sweave")
Sys.setenv(TEXINPUTS=getwd(),
           BIBINPUTS=getwd(),
           BSTINPUTS=getwd())
@

\section{Introduction}
\label{sec:intro}

%MICROARRAY V RNASEQ%
RNA-seq is a recent platform for gene expression data based upon high-throughput sequencing. It has been shown to overcome some of the biases and limitations that are inherent in working with microarray \citep{Zhao2014}, and gives less background noise and provides a greater dynamic range for detection \citep{Hrdlickova2016}. Furthermore, there have been studies to show a strong correlation of results based on RNA-seq and microarray data, suggesting the reproducibility of the utilities of microarray analyses in RNA-seq \citep{Chen2017}.

%CLUSTERING METHODS MICROARRAY%
One area of key interest in cancer genomics is in clustering samples based on subject-specific gene expression profiles. This can provide valuable insight into possible relatedness of cancer samples by grouping similarly expressed samples together. Based on this relatedness, one may be able to make inferences on samples' subtypes of cancer. Subtype discovery has become a field of significant interest because studies have shown that the subtype of a patient's tumor not only affects the prognosis of the disease (Blows et al., 2010), but also the effectiveness of commonly-used treatments like chemotherapy \citep{Carey2011,Mao2017}. Being able to identify the subtype of cancer samples based on gene expression may potentially unlock the door to targeted therapy regimes, in which employed treatments can be more specific to the biology of the particular subtype, and thus more efficient and effective.

While there are numerous methods that have been developed to cluster microarray gene expression data \citep{Xie2007,Monti2003,Kluger2003,Cho2008}, few are able to cluster RNA-seq gene expression. Due to the discrete nature of RNA-seq, the standard Gaussian model cannot be assumed of the underlying distribution, as can be done on microarray by methods like mclust \citep{Yeung2001}. Some current methods get around this by approximating RNA-seq with the normal distribution using some type of transformation \citep{Zwiener2014}. These transformations typically make the data 'more normal' to utilize existing methods for microarray, but there is not one transformation that is superior \citep{Noel-MacDonnell2018}. Thus, analyses using transformed count data would be dependent on the type of transformation used, which raises the question of reproducibility. Also, the few methods that have been developed for clustering raw RNA-seq data have been designed specifically to cluster genes rather than samples \citep{Si2013}.

%POINTS TO ADDRESS IN RNASEQ (normalization,count data,variable selection/prefiltering)%
Biases and artifacts inherent in RNA-seq data also need to be accounted for during clustering. This is typically done using one of an array of normalization steps \citep{Li2015}, although none have been shown to be uniformly superior. Also, the high dimensionality of a genetic dataset necessitates a dimension reduction scheme that can pinpoint genes that may be most likely to be informative in clustering, as a majority of genes may be uninformative and vary due to noise. Thus, many methods use some criterion to pre-filter genes that are nondiscriminatory in nature \citep{Ritter2015}. A widely-used method of pre-filtering is done by ranking and thresholding genes based on its median absolute deviation (MAD) \citep{Chung2008} (\textbf{Alternatives to discuss?}). Noise and other forms of technical variation also intermingles with biological variation, confounding the effects of biological differences. There have been many discussions regarding the irreproducibility of microarray analyses due to noise and batch effects \citep{Scherer2009}, and many attempts have been made to account for these technical variations in RNA-seq, especially in the single cell RNA-seq setting \citep{Brennecke2013,Ding2015}. Sequencing depth is another form of technical variation that describes the dependence of a gene's quantification on its length \citep{Tarazona2011}. This can alter expression profiles dramatically, causing significant findings to be confounded by its sequencing depth.

Additionally, dealing with count data requires different distributional assumptions for model-based methods. The most naive model-based assumption with untransformed count data is the Poisson distribution, which is used alongside a penalized likelihood scheme by a very popular integrative method called iCluster+ \citep{Mo2013}. We believe, however, that such a model also has its limitations, as it does not account for extra-Poisson variation. There are also some non-model-based methods that have been shown to be efficacious with RNA-seq data, such as the average-linkage hierarchical clustering (HC) and K-medoids clustering (K-med) methods \citep{Jaskowiak2018}. HC works by iteratively fusing the two closest clusters together, starting with $n$ clusters and going down in number of clusters. The closeness is determined by a distance matrix and a linkage scheme. Typically, a 1-spearman or 1-pearson correlation is most commonly used, and Jaskowiak proposes that the average-linkage scheme provides the best results with RNA-seq. K-med is very similar in procedure to the widely-known K-means algorithm, but rather than assigning the means as the centroids, K-med assigns datapoints as centroids (medoids). K-med works similarly to K-means by minimizing the distance between cluster members and their corresponding centroid, but is more robust and can better handle count data. Although these do not have a mode of variable selection, they may be useful in clustering nonetheless after some pre-filtering scheme.

%OUR APPROACH%
In this paper, we introduce FSCseq (Feature Selection and Clustering of RNA-seq), in which we propose a finite mixture Negative Binomial model, incorporating a combination of the L2 and SCAD penalties \citep{Fan2001}. By applying this hybrid penalty on the difference in estimated log means, we are able to shrink the cluster parameter estimates closer together, while selecting out genes whose difference in expression across clusters is sufficiently low. The adaptive nature of the SCAD penalty does not introduce as much bias as a classic L1 penalty, while maintaining the thresholding aspect of the lasso. The model is set in a regression scheme to allow for offsetting by normalization factors to adjust for sequencing, and to potentially allow for adjustments for confounders. The estimation is done using an Expectation Maximization (EM) algorithm. The E step weights are passed into the maximization (M step) to allow for cluster-specific estimation, and these weights also provide a very intuitive interpretation as the posterior probabilities of samples being in any particular cluster.

There are four primary steps to our method. First, we pre-filter out genes with low variability using the MAD criterion to remove likely uninformative genes. Second, we select the optimal number of clusters in the Order Selection step using the Bayesian Information Criterion (BIC). Third, we search over a grid of possible combinations of the penalty parameters, and select the optimal tuning parameters using the BIC. Fourth, we input the optimal order and parameters into a full run of our algorithm.

\textbf{Not sure if we do this anymore (below): g vs cl, EM vs CEM}
We also explore an alternative E step update to potentially overcome the limitation of the classic EM of converging to a local maximum. Also, we assess the degrees of freedom in the fitted dispersion parameters per gene. Our framework allows for a separate fitted dispersion per cluster, but a single parameter per gene may suffice and perhaps be more appropriate.

%%%%%%%%%%%%%%%%%%%%%%
%%%%% 2. METHODS %%%%%
%%%%%%%%%%%%%%%%%%%%%%

\section{Methods}
\label{sec:meth}

The main goal of our method is two-fold: 1. to determine discriminatory genes based on differential expression of RNA-seq data, and 2. to cluster subjects based on similar expression profiles of genes. We believe that our method is novel in that it provides a scheme to not only allow for correction for sources of technical variation like sequencing depth, but also to derive posterior probabilities of a sample's subtype based on the model that is fit on the dataset, which may be especially useful in prediction. We utilize an EM algorithm to maximize our likelihood model.

%%% 2.1 Obj fx %%%
\subsection{Likelihood Model}
Raw RNA-seq is count data, so we assume the negative binomial distribution in our regression model. For $Y~NB(\mu,\phi)$, the probability mass function of Y is given by the "NB2" model described by \citet{Hilbe2009}:
\begin{equation}
f(y;\mu,\phi) = \binom{y_i+\phi^{-1}-1}{\phi^{-1}-1} \left[\dfrac{1}{1+\phi\mu_i}\right]^{\frac{1}{\alpha}}\left[\dfrac{\phi\mu_i}{1+\phi\mu_i}\right]^{y_i}
\end{equation}

Now, let $y_{ij}$ be the count of the $i$'th subject for the $j$'th gene, where $i=1,...,n$ and $j=1,...,g$. We model Y by a mixture of K negative binomial distributions, where K represents the total number of underlying clusters in the data. With the assumption that each gene is independent, the density of Y can be written:
\begin{equation}
f(\mathbf{y};\boldsymbol{\mu},\boldsymbol{\phi}) = \prod_{i=1}^n \prod_{j=1}^g \sum_{k=1}^K \pi_k f_{jk}(y_{ij};\mu_{jk},\phi_j)
\end{equation}

where $\sum_{k=1}^K \pi_k = 1$ are the mixing proportions, $\mu_{jk}$ is the mean of cluster k in gene j, and $\phi_j$ is the dispersion parameter of gene j. The link and variance functions are given as follows:
\begin{align}
log_2(\mu_{ijk}) = & \beta_{jk} + s_i \\
V(y_{jk}) = & \mu_{jk} + \phi_j\mu_{jk}^2
\end{align}

where $\beta_{jk}$ represents the cluster log base 2 (log2) mean (henceforth called "cluster log means") for gene j and cluster k, and $s_i$ is the log2 of the subject-specific normalization factors calculated by DESeq2 \citep{Love2014}. We pass these normalization factors here as offsets into our regression model to correct for sequencing depth. Note that the equation calls for one common dispersion parameter per gene. The analog that we will assess is the use of separate cluster-specific dispersions, which replaces $\phi_j$ with $\phi_{jk}$ above. It is not known which scheme is more appropriate for dispersions in real data. Many prominent methods of analyzing gene expression like DESeq2 use gene-specific dispersion parameters to account for extra-Poisson variation. However, it can be argued that in cases where the differences in dispersion between clusters is very large, estimating additional cluster-specific dispersion parameters may be required to better fit the model. However, this may lead to overfitting and skew the clustering results. In this paper, we find that one common dispersion per gene tends to be more accurate in terms of performance, but we will include the option to use distinct cluster-specific dispersions per gene as well.

% 2.1.1 Penalty
\subsubsection{Penalty}
The penalty we incorporate is a modified version of the grouped truncated lasso penalty, which uses a hybrid of the lasso (L1) and ridge (L2) penalties \citep{Pan2013}. Instead of the lasso, we utilize the Smoothly Clipped Absolute Deviation (SCAD) penalty, which is shown to introduce less bias than the classic L1 penalty, while still being able to threshold coefficients to zero. We incorporate this hybrid penalty by adding an additional parameter to balance between the two penalty terms, much like $\alpha$ in the elastic net penalty \citep{Zou2005}. In this context, rather than penalizing coefficients ($\beta$) to zero like in a typical regression setting, we penalize the differences between log2 cluster means (denoted by $\theta$) to zero.
\begin{equation}
p_{\lambda}(\beta)=\frac{\lambda(1-\alpha)}{2}\sum_{k<l} \norm{\beta_k-\beta_l-\theta_{kl}}_2^2 + \lambda\alpha \sum_{k<l}SCAD(\norm{\theta_{kl}}_2)
\end{equation}

where $\theta_{kl}=\beta_k-\beta_l$ is a reparametrization to store the difference between estimated cluster log means in the previous iteration, and $SCAD(\theta)=\$ \textbf{Insert form of SCAD penalty here}. In this penalization scheme, $\lambda$ controls the amount of penalization introduced, and $\alpha$ parameter balances between the L2 and SCAD penalties. Note that under this penalty scheme, the difference in log means can never be set exactly to zero. However, the $\theta_{kl}$ parameter would be thresholded to zero when the log mean of cluster $k$ and $l$ are sufficiently close, which would indicate that the respective gene is nondiscriminatory across these two clusters.

%%% 2.2 Computation %%%
\subsection{Computation}

Estimation of $\beta_{jk}$ and $\phi_j$ is done by maximizing the conditional expectation of the complete data log-likelihood function, which is given by
\begin{equation}
log[L(\Psi)]=\sum_{i=1}^{n} \sum_{k=1}^{K} z_{ik} \{log(\pi_k)+log[f_k(\boldsymbol{y_i}; \boldsymbol{\beta_k},\boldsymbol{\phi})]\}+p_\lambda(\beta)
\end{equation}

where $z_{ik}=I(z_i=k)$ denotes the indicator of subject $i$ being in cluster $k$ and $f_k(\boldsymbol{y_i}; \boldsymbol{\beta_k})=\prod_{j=1}^{g} f_{jk}(y_{ij}; \beta_{jk},\phi_j)$. The cluster proportions are given by $\pi_k$. Because $z_{ik}$ is unobservable, we estimate this quantity by the conditional expectation of $z_{ik}$ given the parameters estimated from the $m$th step. For simplicity of the model, we assume that the expression of each gene is uncorrelated to that of any other gene, which makes the maximization of $\beta_{jk}$ and $\phi_j$ separable. We can then implement a gene-by-gene maximization procedure, which we outline in the M step section.

% 2.2.1 E Step
\subsubsection{E Step}
We calculate the conditional expectation of the $z_{ik}$ given the current estimates of the parameters. We denote $\hat{z}_{ik}^{(m)} = E[z_{ik} \mid \mathbf{y}, \boldsymbol{\hat{\beta}^{(m)}}, \boldsymbol{\hat{\pi}^{(m)}}]$. Another way to think about this quantity is as the posterior probability of subject $i$ being in cluster $k$. These quantities are passed through as weights in the estimation of the coefficients in the maximization step. The update on these weights are:

\begin{equation}
\hat{z}_{ik}^{(m)}=\dfrac{\hat{\pi}_k^{(m)}f_k(\boldsymbol{y_i};\boldsymbol{\hat{\beta}_k^{(m)},\hat{\phi}_k^{(m)}})}{\sum_{l=1}^{K}\hat{\pi}_l^{(m)}f_l(\boldsymbol{y_i};\boldsymbol{\hat{\beta}_l^{(m)},\hat{\phi}_l^{(m)}})}
\end{equation}

At convergence, we are able to assign patients according to these posterior probabilities. These probabilities will also allow for partial cluster membership, as their values will be between 0 and 1, but the final cluster label is assigned to the cluster that corresponds to the maximum value of the posterior probabilities for that subject.

The EM algorithm is known to be able to converge to the local maximum or saddle point rather than the global maximum \citep{Yu2018}. To prevent this, we compare many initializations with a shorter EM run. Then, we choose the optimal initialization based on the BIC.

We also tried replacing the E step update with the classification E step update. This method (called CEM) has been shown to help the algorithm to not get stuck at a local maxima \citep{Biernacki2003}, but we found that the convergence of the CEM took much longer, and was not feasible to select across many initializations in higher dimensions. The CEM by itself (without searching multiple initializations) did not reliably attain the global maximum, so we also attempted searching initializations via EM, then using CEM in the final run. However, the CEM often knocked the good starting point back into a local maximum. Thus, we determined that a scheme with short EM runs with a final EM run yielded the best results in this setting. However, we leave the CEM as an alternative to the EM in our package.

% 2.2.2 M Step
\subsubsection{M Step}
In the M step of the EM algorithm, we update the current estimates of the parameters to maximize the penalized objective function. The maximization of $\pi_k$ and $\beta_{jk}$ are separable, thus they can be done independently.

%Est of beta
The maximization of the cluster means and dispersion parameter(s) is performed by iteratively reweighted least squares using a coordinate descent algorithm. We first maximize the penalized objective function to estimate the cluster means. This is accomplished by using a transformed response \citep{Breheny2011}. We fix a gene $j$, and transform the responses by the following:

$$\tilde{y}_{ik}=\hat{\eta_k}+(\frac{y_i-g(\hat{\eta}_k)}{g'(\hat{\eta}_k)})$$

Here, $g()$ refers to the inverse link function for the Negative Binomial family with log link, thus $g(\eta)=\mu$, where $g^{-1}(\mu)=log(\mu)$ is the standard log link function.

Using this transformation, the penalized objective function becomes:

\begin{equation}
Q_j(\boldsymbol{\beta}) \approx \frac{1}{2n}(\mathbf{\tilde{y}}-\mathbf{X}\boldsymbol{\beta})'\mathbf{W}(\mathbf{\tilde{y}}-\mathbf{X}\boldsymbol{\beta})+\sum_{k=1}^{K}p_{\boldsymbol{\lambda}}(\beta_{jk})
\end{equation}

where X is an $(nK)$x$K$ matrix of 1's and 0's that represent the indicator of each cluster for each transformed response, and W is the matrix of the E step weights. In this framework, we pass on $\hat{z}^{(m)}_{ik}$ as the weights in regressing $\tilde{y}_{ik}$ on X.

Therefore, we have $w_{ik}=\sqrt{\dfrac{\hat{z}^{(m)}_{ik} g'(\eta)^2}{V(\mu)}}$, where the variance is $V(\mu)=\mu + \phi\mu^2$ for the Negative Binomial. Equation 3 is maximized for each value of $j=1,...,g$. The update equations become:

\begin{equation}
\begin{split}
\hat{\beta}_k^{(m+1)}&=\dfrac{\frac{1}{n}\sum_{i=1}^{n}w_{ik}\tilde{y}_{ik}+\lambda(1-\alpha)[\sum_{l>k}(\hat{\beta}_l^{(m)}+\hat{\theta}_{kl}^{(m)})+\sum_{l<k}(\hat{\beta}_l^{(m+1)}-\hat{\theta}_{lk}^{(m)})]}{\lambda(1-\alpha)(K-1)+\frac{1}{n}\sum_{i=1}^{n}w_{ik}} \\
\hat{\theta}_{kl}^{(m+1)}&=\begin{dcases} 
      sgn(\hat{\theta}_{kl}^{(m)})(\norm{\hat{\theta}_{kl}^{(m+1)}}-\alpha\lambda) & \norm{\hat{\theta}_{kl}^{(m)}} \leq 2\lambda\alpha \\
      \frac{(a-1)\hat{\theta}_{kl}^{(m)} - sgn(\hat{\theta}_{kl}^{(m)})a\lambda\alpha}{(a-2)} & 2\lambda\alpha < \norm{\hat{\theta}_{kl}^{(m)}} \leq a\lambda\alpha \\
      \hat{\theta}_{kl}^{(m)} & \norm{\hat{\theta}_{kl}^{(m)}} > a\lambda\alpha
   \end{dcases}
\end{split}
\end{equation}

with $ST(\alpha,\lambda)=sign(\alpha)(\lvert \alpha \rvert-\lambda)_+$ denoting the usual soft-thresholding rule. The gene $j$ is fixed, and the above procedure is repeated for every gene $j=1, ..., g$. In this framework, $\hat{\theta}$ represents shrinkage on the cluster means, with the cluster means set as equal when they are within $\lambda_2$ from each other. After sequentially updating the cluster means, we estimate the overdispersion parameter $\phi$ using a maximum likelihood (ML) approach. However, this approach is limited due to instability when sample size is small. To mitigate this, we include a very small penalty (order of $10^{-50}$) on the ML estimation of $\phi$ to stabilize the estimate in the low sample setting. We also compared the effects of fitting a common dispersion per gene, and cluster-specific dispersions per gene. It is not known which would be more appropriate for this type of data. Cluster-specific dispersion parameters may provide a better fitting model to the data, but gene-specific dispersion parameters will allow for more sparsity and avoid the issue of overfitting. The function in the R package will include the option to use either scheme.

% Convergence
The stopping criterion for the EM algorithm is based on a threshold on the Q function. The algorithm is considered to have "converged" if $\lvert Q^{(m+1)}-Q^{(m)} \rvert< \epsilon_1$

The stopping criterion for the IRLS in the M step is based on a threshold on the sum of squares of the parameters. The algorithm is considered to have converged if $\lvert \boldsymbol{\beta}^{(l+1)}-\boldsymbol{\beta}^{(l)} \rvert + \lvert \boldsymbol{\phi}^{(l+1)}-\boldsymbol{\phi}^{(l)} \rvert < \epsilon_2$

We set $\epsilon_1 = 10^{-12}$ and $\epsilon_2 = 10^{-6}$.

% 2.2.3 Tuning Parameters
\subsubsection{Tuning Parameters}

The optimal number of clusters K is found by searching over a range of values $K=(1, ...,7)$ and comparing the fit using the yielded BIC values. \ref{fig:1} gives a graphical representation of the procedure on an example simulated dataset

\begin{figure}
\begin{center}
%\includegraphics{C:/Users/limdd/Documents/Research/Sweave/Project1/BICvOrder.png}
<<echo=FALSE,fig=TRUE>>=
load("C:/Users/limdd/Documents/Research/Sweave/Project1/list_BIC_n100_g865_K3.Rout")
plot(list_BIC,ylab="BIC",xlab="K",main="Plot of BIC vs. order")
@
\end{center}
\caption{Order selection is done by choosing the order (K) that minimizes the BIC. Here, the true order is K = 3 in a simulated dataset of n = 100 and g = 835 (after pre-filtering low count genes), and 20 \% discriminatory genes with log fold change of 2.}
\label{fig:1}
\end{figure}

Optimal tuning parameters for \(\lambda_2\) and \(\tau\) are also found by using the BIC criterion. The algorithm searches over a grid of values, and selects the combination that yields the lowest BIC. Pan proposed fixing the shrinkage parameter $\lambda_1=1$, varying the thresholding parameters $\lambda_2$ and $\tau$. We set $\lambda_1=1$ and search over $\lambda_2=(0.01, 0.05, 0.1, 0.15, 0.2)$ and $\tau=(0.1, 0.3, 0.5, 0.7, 0.9)$. Pan further proposed re-running the algorithm after doubling the value of $\lambda_1$ upon convergence in order to prevent the algorithm from being stuck at local maxima. For the sake of computation time, we instead implemented the classification and stochastic E step weights as an alternative solution to this issue.

\subsubsection{Prediction}
(INCLUDE DESCRIPTION OF PREDICTION)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% 3. Numerical Examples %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Numerical Examples}
\label{sec:examples}

%%% 3.1 Simulations %%%
\subsection{Simulations}

It is well-known that a majority of the genes in a typical dataset is nondiscriminatory across subtypes. In order to cluster subtypes accurately, it is important to filter out genes that are obviously nondiscriminatory in nature. One common way to do this is by filtering out genes whose median absolute deviation is low \citep{Chung2008}. We utilize this, along with a subsequent wrapper in our method to perform pre-filtering and variable selection on the data for analysis \citep{Ritter2015}.

(MAYBE ADD: NB GOF pvalue vs MAD)

We simulated 24 datasets per condition, and spanned an extensive set of conditions. Variable conditions included log fold change ($LFC$), number of clusters ($K$), number of samples per cluster ($n_k$), number of genes ($g$), and proportion of discriminatory genes ($p_{disc}$). We searched over $LFC=(1, 2)$, $K=(2, 3, ..., 7)$, $n_k=(25, 50)$, $g=2000$, and $p_{disc}=0.05$

To explore the behaviors in various realistic settings, we simulated low, medium, and high expression and noise values based on the 25th, 50th, and 75th quantiles of the estimates of expression and dispersion in the TCGA Breast Cancer dataset. These values were approximately $\beta=(3.75,6.5)$ and $\phi=(0.15,0.35,0.50,1.00)$, respectively.

After simulating, we imposed pre-filtering and kept just the genes with the top 25\% of MAD. \ref{tab:1} shows the performance statistics with varying coefficients and noise, averaged across all of the other variables ($LFC$, $K$, $n_k$, $g$, and $p_{disc}$). One very promising added utility of our method is the ability to perform prediction based on the estimated parameters and the posterior weights from the E step. 

We also compare with three competing methods: iCluster+, average-linkage hierarchical clustering (HC), and K-medoids (K-med). Order selection for HC was done by using the NbClust package \citep{Charrad2014}, which compares thirty different techniques of order selection for agglomerative/K-means clustering methods. The optimal order was selected based on which value was selected most often by these methods. Order selection for K-medoids was done by selecting the order which maximizes the silhouette value. In this procedure, $K=1$ was excluded because it is not possible to derive a silhouette value with just one cluster.

Order selection for iCluster+ was difficult to automatize, as the paper suggests looking graphically for a plateau of the deviance ratio (\% Variability Explained) as the the optimal number of clusters. Mo suggests in the manual of the iClusterPlus package that for increased noise in the dataset, the deviance ratio will continue to increase with higher order, so the highest deviance ratio will almost always be at the highest number of clusters. In order to systemize the procedure across numerous simulation cases, we selected an arbitrary threshold of 0.05, such that if the percent increase in variability explained is less that 0.05 with an added cluster, it is deemed insignificant, and the order is selected as the immediately previous number. We analyze the performance of clustering, discovering discriminatory genes across clusters, as well as predicting on newly simulated samples (floor(0.1*n)) in \ref{tab:1}.

<<xtable12, results=tex, echo=FALSE>>==
library(xtable)
#load("C:/Users/limdd/Documents/Research/Simulations/gene_fixed_mad25_icluster_and_pred/second/sim_res_tab1.RData")
#load("C:/Users/limdd/Documents/Research/Simulations/filtmad20/sim_res.RData")
load("C:/Users/limdd/Documents/Research/Simulations/elasticSCAD/24sims/sim_res.RData")

#head(table1)
colnames(table1) = c("$\\beta$","$\\phi$","LFC","$n_{k}$","K","$\\hat{K}$","OA","$p_{disc}$","$\\hat{p_{disc}}$","ARI","sens","FPR","MADsens","MADFPR","PA","iK","iARI","hcK","hcARI","kmK","kmARI")

table1_EM_low=table1[c(1:16,33:48),1:15]
table1_EM_hi=table1[c(17:32,49:64),1:15]
table1_other_low=table1[c(1:16,33:48),c(1:6,16,18,20,10,17,19,21)]
table1_other_hi=table1[c(17:32,49:64),c(1:6,16,18,20,10,17,19,21)]

rws <- rep(c(1:4),times=4)+rep(c(4,12,20,28),each=4)-1
col <- rep("\\rowcolor[gray]{0.95}", length(rws))

tab1 = xtable(table1_EM_low,
              digits=2,
              table.placement="!h",
              caption="Effects of high/low simulated log means ($\\beta = 3.75$ and $6.50 $) with low and medium noise ($\\phi = 0.15 and 0.35$) in simulated datasets on performance. Each row is based on 24 simulated datasets. Order Accuracy (OA) is the proportion of datasets that correctly selected the true K (order), and prediction accuracy (PA) is the proportion of new simulated subjects that were correctly clustered. PA takes into account only runs that correctly selected order, and missing values denote no dataset correctly selected the true K.",
              label = "tab:1",na.print="")      # K=3, n=50, g=1000
print(tab1,include.rownames=F, sanitize.text.function=identity,
              floating.environment = "sidewaystable",add.to.row=list(pos=as.list(rws),command=col))

tab2 = xtable(table1_EM_hi,
              digits=2,
              table.placement="!h",
              caption="Effects of high/low simulated log means ($\\beta = 3.75 and 6.50$) with high and very high noise ($\\phi = 0.50$ and $1.00$) in simulated datasets on performance.",
              label = "tab:2",na.print="")      # K=3, n=50, g=1000
print(tab2,include.rownames=F, sanitize.text.function=identity,
              floating.environment = "sidewaystable",add.to.row=list(pos=as.list(rws),command=col))

@

As the average expression of the genes ($\beta$) goes up, we see that clustering performance (ARI) and order selection accuracy both increase. This is because as the baseline expression for a gene increases, the distance betweentwo clusters in that particular gene increases, since the log fold change creates a greater separation at higher expressions. For example, the distance between $\beta=3$ and $\beta=4$ is much smaller than the distance between $\beta=6$ and $\beta=7$, since $\mu=e^{\beta}$, and $e^4-e^3 < e^7-e^6$.

Conversely, as noise ($\phi$) increases, we see a decrease in clustering and order selection accuracy. The noisier the dataset, the harder it will be to distinguish whether the observed differential expression has biological significance, or is simply due to randomness in  the data. 

Another useful feature of our algorithm is the ability to perform simultaneous variable selection while clustering. This helps us uncover which genes are discriminatory across clusters. We note that the sensitivity, or the proportion of discriminatory genes that are determined correctly to be discriminatory, is very good when the noise is small, but gets worse as noise is increased. We found that the false positive rate (FPR), or the rate at which nondiscriminatory genes are falsely determined to be discriminatory, does not follow this trend, but remains relatively constant through varying noise. We also observed that sensitivity and FPR are both significantly better when $K=4$ vs. when $K=2$. This is because for more simulated clusters, the log2 fold change is applied multiple times, making it easier for the algorithm to distinguish genes that are differentially expressed across these clusters.

% It is important to note that in real applications, the differential expression may not be present between every two adjacent clusters. For example, for one particular gene, the LFC may be 1.5 between clusters 1 and 2, but 0 between clusters 2, 3, and 4.

A common integrative clustering method that is able to handle count data is iCluster+. It assumes a Poisson distribution on the data, and uses a L1 penalized likelihood model to estimate the parameters, while using a sampling scheme from the posterior distribution of the latent variables to discover clustering labels. We compared our method to just the count data portion of iCluster+'s integrative method.

Also, Two other clustering methods that have been found to be very useful for clustering RNA-seq count data are average-linkage hierarchical clustering (HC) and K-medoids (KM). There has been a study that suggests that these were among the most robust and efficient in clustering this type of data. Order selection for HC was done by selecting the order that was most often selected by the 30 different methods available via the NbClust package. Order selection for K-medoids was performed by maximizing the average silhouette value. Silhouette values are not applicable for $K=1$, so the order search for K-medoids omitted the case with just one cluster. \ref{tab:3} and \ref{tab:4} show side-by-side comparisons of order selection and clustering performances for the EM, iCluster+, HC, and KM.

<<xtable34, results=tex, echo=FALSE>>=
tab3 = xtable(table1_other_low,
              digits=2,
              table.placement="!h",
              caption="Effects of high/low simulated log means ($\\beta = 3.75$ and $6.50$) with low and medium noise ($\\phi = 0.15$ and $0.35$) in simulated datasets on performance, compared to competing methods average-linkage hierarchical clustering (HC) and K-medoids (KM).",
              label = "tab:3",na.print="")      # K=3, n=50, g=1000

print(tab3,include.rownames=F, sanitize.text.function=identity,
              floating.environment = "sidewaystable",add.to.row=list(pos=as.list(rws),command=col))

tab4 = xtable(table1_other_hi,
              digits=2,
              table.placement="!h",
              caption="Effects of high/low simulated log means ($\\beta = 3.75$ and $6.50$) with high and very high noise ($\\phi = 0.50$ and $1.00$) in simulated datasets on performance, compared to competing methods HC and KM.",
              label = "tab:4",na.print="")      # K=3, n=50, g=1000

print(tab4,include.rownames=F, sanitize.text.function=identity,
              floating.environment = "sidewaystable",add.to.row=list(pos=as.list(rws),command=col))
@

Here, we introduce some variants of our method. For one, we have so far restricted the simulations to contain gene-specific dispersions. It is not currently known whether a scheme with one dispersion parameter across all clusters for each gene adequately portrays real data. It may sometimes be more appropriate to introduce cluster-specific dispersion parameters to better fit the data, yielding more accurate estimates. However, this may also result in overfitting the data, which would cause our model to get stuck at a local maxima. One potential drawback that may result is that the order may be underestimated, as the extra dispersion parameters may cause the algorithm to favor a smaller number of clusters in the data.

Secondly, prior work has shown that the EM algorithm tends to converge to the local maximum, rather than the global maximum. Thus, this may result in the algorithm being "stuck" at less than ideal conditions. One way to prevent this is to replace the current E step with the classification E step. This variant of the E step introduces some randomness by drawing the posterior probabilities closer together, causing the algorithm to overcome local maxima through small perturbations of the clustering index. We use a simulated annealing method \citep{Rose1998,Si2013} to slowly wean off the noise-adding effects of the CEM, and revert to the original EM. \ref{tab:2} shows the results of these variants under simulated gene-specific and cluster-specific dispersions, respectively, as well as the analogous iCluster+ results. Results are based on 10 simulated datasets for each case. (1) gEM is the EM with gene-specific dispersions, as before; (2) gCEM is the CEM with gene-specific dispersions; (3) clEM is the EM with cluster-specific dispersions; and (4) clCEM is the CEM with cluster-specific dispersions.

%%%%% EXCLUDE ICLUSTER+ COMPARISON FOR TABLES 3 AND 4. JUST COMMENT ON IT IN PAPER %%%%%%%%%%%%%
<<xtable5,results=tex,echo=FALSE>>==
#load("C:/Users/limdd/Documents/Research/Simulations/g_v_cl__EM_v_CEM/gvcl_EMvCEM.RData")
#colnames(tab_g) = c("$K$","ARI","$p_{disc}$","Sensitivity","FPR","Pred Acc")


# colnames(table2) = c("$K$","OA","ARI","$p_{disc}$","Sens","FPR","PA")
# tab5 = xtable(table2,
#               digits=2,
#               table.placement="!h",
#               caption="Clustering, discovery, and prediction performance of variants of EM. Also, the clustering performances of iCluster+, average-linkage hierarchical clustering, and K-medoids. Note: For fair comparisons, ARI values are based on runs with true number of clusters. The simulated dispersions were gene-specific.",
#               label = "tab:5",na.print="",floating.environment = "table*")      # K=3, n=50, g=1000jo
# print(tab5,include.rownames=T, sanitize.text.function=identity)

@

We also notice the effect of the classification E step modification in our algorithm. We see that for a gene-specific dispersion scheme, the CEM seems to increase the ARI slightly. We notice that in \ref{tab:2}, the CEM caused the cluster-specific dispersion EM model to select the incorrect order. This portrays a risk of the CEM, as the variant E step adds randomness by drawing posterior probabilities closer together, and may lead to such instabilities.

Although the clustering accuracy was dependent on the dispersion scheme, the sensitivity and false positive rate in discovering discriminatory genes seem to not be affected. In each case, the algorithm was able to correctly distinguish all of the simulated discriminatory genes, with very few false positive cases.

The prediction accuracy was calculated by first inputting the correct order in cases that the order was incorrectly selected, simulating 20 samples ($n_{pred}=0.1n$) with the same simulated parameters, and selecting the cluster with the highest posterior probability for each sample. In either intrinsic dispersion scheme, we see that the prediction accuracy is very high (> 0.94).

%%% 3.2 Real Data %%%
\subsection{Real Data}

\subsubsection{TCGA Breast Cancer Dataset}

We performed our clustering analysis on The Cancer Genome Atlas (TCGA) breast cancer (BRCA) dataset. This dataset contained 1215 subjects and 21022 genes. Purity is known to be a very strong confounder for cancer genomic data. Thus, we narrowed down our analysis to only samples whose purity estimate was greater than 0.9. We were left with 132 samples with 5 different subtypes: 53 luminal A, 38 luminal B, 32 basal-like, 6 HER2-enriched, and 2 normal-like. Then, we filtered out genes with low count by excluding genes with MAD scores below the median MAD from our analysis, yielding 9062 total genes. We compared our clustering results with the clusters that have been previously determined and annotated (Koboldt et al, 2012). These annotated subtypes were results based on the well-known PAM50 genes.

We also compared our method to the count data method of iCluster+, as well as the average-linkage hierarchical clustering (HC) model and K-medoids (K-med) model. We used the same criterion (BIC) to search for the optimal order and tuning parameters. The order selection steps for the EM and for iCluster+ were compared to see if the annotated number of clusters can be recovered. The EM correctly recovered the annotated order of $K=4$

% Not sure if should include silhouette
Mean silhouette value measures clustering accuracy by a combination of proximity within clusters, and distance away from other clusters \citep{Ge2017}. A recent study showed that higher average silhouette value is robust to small sample size, and can be very indicative of degree of cluster separation in RNA-seq data \citep{Zhao2018}. K-med works by minimizing the distance between each cluster point and the center of the cluster. This seems to most closely reflect how the silhouette value is computed, so K-med naturally yields the highest mean silhouette value ($0.136$). After K-med, EM yielded the highest mean silhouette value ($0.100$) Silhouette values were based on the 1-pearson correlation distance.

We also performed survival analysis with each respective clustering labels. Although the results seem to slightly favor the annotated clusters ($p=0.125$) over the EM ($p=0.200$), survival data was very limited in this study (only 79 out of 513 samples), so we were hesitant to draw any conclusions. One way we mitigate the problem of low sample size in our survival analysis is by using a permutation-based test to yield an empirical p-value, rather than the classic log-rank test \citep{Rappoport2018}. We then conducted the Fisher's Exact test on the overall survival (OS) of the patients. There were 62 OS events, and no missing data points in the 513 total samples.

HC clusters seemed to yield the lowest p-value in this case, but upon closer examination, we found that 509 out of the 513 samples had been clustered together. This is clearly not optimal, and showed that the hierarchical model was unable to distinguish the clusters very well. The annotated clusters surprisingly yielded the highest p-value of $0.579$, while iCluster+ yielded a smaller p-value of $0.291$. We found a significantly smaller p-value of $0.088$ via the EM.

<<xtable3,results=tex,echo=F>>=
load("C:/Users/limdd/Documents/Research/Real Data/TCGA BRCA/BRCA_compare_res.RData")
colnames(tab) = c("K","ARI","Silhouette","L-R p-value","Fisher's p-value")

tab6 = xtable(tab,digits=3,caption="Cluster Analyses and p-values from Log-Rank survival test (L-R) and Fisher's Exact test of TCGA Breast Cancer dataset on NB-EM, iCluster+, Average-linkage hierarchical clustering (HC), and K-medoids (K-Med).",
              label="tab:6",table.placement="H",na.print="",
              floating.environment = "table*")
print(tab6,sanitize.text.function=identity,table.placement="H")

@

Our method produced a total of 2988 discriminatory genes. \ref{fig:2} shows a heatmap of these genes, ordered by the derived clusters. Given the very small false positive rate in our simulations, we predict that many of these genes provide valuable information in differential expression across these clusters.

\begin{figure}
\begin{center}
\includegraphics[width=170mm]{C:/Users/limdd/Documents/Research/Sweave/Project1/BRCA_HM_disc.eps}
\end{center}
\caption{Heatmap of derived discriminatory genes. Cluster labels are annotated above, with derived clusters ("EM") and annotated clusters.}
\label{fig:2}
\end{figure}

It has been widely known and accepted that the PAM50 genes are genes of significant interest in determining subtypes of cancer. We thus performed further analyses on just these PAM50 genes. Interestingly, 22 of the 50 genes were pre-filtered out due to a low MAD value. Also, out of the 28 that made it past the pre-filtering step, our algorithm found only 20 genes to be discriminatory across clusters. \ref{fig:3} shows the distribution of MAD scores of the PAM50 genes that were pre-filtered out, as well as the estimated log fold change across clusters for those PAM50 genes that were selected by our algorithm to be nondiscriminatory. Because these genes yielded a smaller estimated LFC (< 0.5), the penalty scheme selects them out as nondiscriminatory.

\begin{figure}
\begin{center}
\includegraphics[width=170mm]{C:/Users/limdd/Documents/Research/Sweave/Project1/BRCA_MAD_LFC_PAM50.png}
\end{center}
\caption{(left) MAD scores of PAM50 genes, stratified by inclusion via MAD pre-filtering scheme; (right) Estimated LFC's of normalized counts across derived clusters}
\label{fig:3}
\end{figure}

The low variation in the pre-filtered out PAM50 genes may suggest that these genes are not as valuable in clustering RNA-seq data. Even still, our algorithm selected about $15.2\%$ of the initial genes to be discriminatory, and selected $40\%$ of the PAM50 genes as discriminatory. \ref{fig:4} displays the heatmaps of these PAM50 genes, ordered by the derived and annotated cluster labels.

\begin{figure}
\begin{center}
\includegraphics[width=170mm]{C:/Users/limdd/Documents/Research/Sweave/Project1/BRCA_HMs_PAM50.jpg}
\end{center}
\caption{Heatmap of PAM50 genes ordered by derived (top) clusters, and by annotated (bottom) clusters.}
\label{fig:4}
\end{figure}

We found that the the derived clusters tended to fuse together the Luminal A and Luminal B subtypes. Also, the algorithm was not able to cluster out the HER2-enriched subtype distinctly from the other subtypes, but samples of that subtypes seems to be interspersed throughout. The Basal-like and Luminal A subtypes seem to, at least for the most part, be distinguishably clustered together, but the Luminal B and HER2 clusters were not clustered as well apart.

\subsubsection{TCGA Bladder Cancer Dataset}

We also performed similar analyses on the TCGA Bladder Cancer dataset. This dataset contained 408 samples with annotated subtypes based on a study (Robertson et al, 2017): 142 basal-squamous, 142 luminal-papillary, 78 luminal-infiltrated, 26 luminal, and 20 neuronal. As before, we filtered just the genes with the top 25 quantile MAD scores, leaving 4916 genes in the analysis. The luminal and neuronal subtypes accounted for just 6.4\% and 4.9\% of the subjects, respectively.

The EM order selection step yielded $K=4$, while the iCluster+ order selection step yielded $K=7$. We see that iCluster+ constantly overestimates the order because of its inability to deal with extra-Poisson variation. Our order selection was closer to the truth ($K=5$), but it slightly underestimated the order because of the small sample size in the luminal and neuronal subtypes. For comparisons, we input the correct number of clusters to each method.

<<xtable4,results=tex,echo=F>>=
load("C:/Users/limdd/Documents/Research/Real Data/TCGA BLCA/BLCA_compare_res.RData")
colnames(tab) = c("K","ARI","Silhouette","L-R p-value","Fisher's p-value")

tab7 = xtable(tab,digits=3,caption="Cluster Analyses and p-values of TCGA Bladder Cancer data from Log-Rank survival test (L-R) and Fisher's Exact test on NB-EM, iCluster+, Average-linkage hierarchical clustering (HC), and K-medoids (K-Med).",
              label="tab:7",table.placement="H",na.print="",floating.environment = "table*")
print(tab7,sanitize.text.function=identity,table.placement="H")

@

Survival analysis was done on 178 non-missing survival times. Surprisingly, the EM produced a lower ARI value ($0.335$) with the correct order than with the optimal order from the order selection step ($0.407$), although the average silhouette value with the correct order ($0.077$) was slightly higher than with the selected order ($0.067$). 

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% 4. DISCUSSION %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
\label{sec:disc}

Our findings from the simultions and real data applications give evidence to the usefulness of our method.

One way that 
Extensions: gene-gene interaction/correlation, adjusting for confounders (batch effects),

\backmatter
%  This section is optional.  Here is where you will want to cite
%  grants, people who helped with the paper, etc.  But keep it short!

\section*{Acknowledgements}

The authors thank Professor A. Sen for some helpful suggestions,
Dr C. R. Rangarajan for a critical reading of the original version of the
paper, and an anonymous referee for very useful comments that improved
the presentation of the paper.\vspace*{-8pt}

%  If your paper refers to supplementary web material, then you MUST
%  include this section!!  See Instructions for Authors at the journal
%  website http://www.biometrics.tibs.org

\section*{Supplementary Materials}

Web Appendix A, referenced in Section~\ref{s:model}, is available with
this paper at the Biometrics website on Wiley Online
Library.\vspace*{-8pt}

\section{References}
%  Here, we create the bibliographic entries manually, following the
%  journal style.  If you use this method or use natbib, PLEASE PAY
%  CAREFUL ATTENTION TO THE BIBLIOGRAPHIC STYLE IN A RECENT ISSUE OF
%  THE JOURNAL AND FOLLOW IT!  Failure to follow stylistic conventions
%  just lengthens the time spend copyediting your paper and hence its
%  position in the publication queue should it be accepted.

%  We greatly prefer that you incorporate the references for your
%  article into the body of the article as we have done here 
%  (you can use natbib or not as you choose) than use BiBTeX,
%  so that your article is self-contained in one file.
%  If you do use BiBTeX, please use the .bst file that comes with 
%  the distribution.  In this case, replace the thebibliography
%  environment below by 

\bibliographystyle{biom}
\bibliography{Proj1}
%\bibliography{proj1bib}

\appendix
%  To get the journal style of heading for an appendix, mimic the following.

\section{}
\subsection{Title of appendix}

Put your short appendix here.  Remember, longer appendices are
possible when presented as Supplementary Web Material.  Please 
review and follow the journal policy for this material, available
under Instructions for Authors at \texttt{http://www.biometrics.tibs.org}.

\label{lastpage}

\end{document}